{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ray RLlib 노트북 (파이프 오르기 전용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요 패키지 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "from horcrux_terrain_v1.envs import PlaneWorld\n",
    "from horcrux_terrain_v1.envs import PlanePipeWorld\n",
    "from horcrux_terrain_v1.envs import ClimbWorld\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "\n",
    "from ray.tune.registry import register_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray 실행 (Warning 관련 무시 키워드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(runtime_env={\"env_vars\": {\"PYTHONWARNINGS\": \"ignore::DeprecationWarning\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym -> Rllib Env 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"forward_reward_weight\": 6.5,\n",
    "}\n",
    "\n",
    "# Sand\n",
    "register_env(\"sand-v1\", lambda config: SandWorld(forward_reward_weight=env_config[\"forward_reward_weight\"],))\n",
    "\n",
    "# Plane\n",
    "register_env(\"plane-v1\", lambda config: PlaneWorld(forward_reward_weight=env_config[\"forward_reward_weight\"],))\n",
    "\n",
    "# Pipe\n",
    "register_env(\"pipe-v1\", lambda config: PlanePipeWorld(forward_reward_weight=env_config[\"forward_reward_weight\"],))\n",
    "\n",
    "# Climb\n",
    "register_env(\"climb-v1\", lambda config: ClimbWorld(forward_reward_weight=env_config[\"forward_reward_weight\"],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 알고리즘 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SACConfig()\n",
    "# Activate new API stack. -> 구려서 안씀.\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=False,\n",
    "    enable_env_runner_and_connector_v2=False,\n",
    ")\n",
    "config.environment(\"climb-v1\")\n",
    "config.framework(\"torch\")\n",
    "total_workers = 12\n",
    "config.resources(num_gpus=1,num_cpus_per_worker=1, num_gpus_per_worker= 1/(total_workers+1))\n",
    "config.rollouts(num_rollout_workers=total_workers)\n",
    "config.training(\n",
    "    gamma=0.95, \n",
    "    # kl_coeff=0.3, \n",
    "    replay_buffer_config = {\n",
    "            \"_enable_replay_buffer_api\": True,\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": int(1e6),\n",
    "            # If True prioritized replay buffer will be used.\n",
    "            \"prioritized_replay\": False,\n",
    "            \"prioritized_replay_alpha\": 0.6,\n",
    "            \"prioritized_replay_beta\": 0.4,\n",
    "            \"prioritized_replay_eps\": 1e-6,\n",
    "            # Whether to compute priorities already on the remote worker side.\n",
    "            \"worker_side_prioritization\": False,\n",
    "        },\n",
    "\n",
    "    # See model catalog for more options.\n",
    "    # https://docs.ray.io/en/latest/rllib/rllib-models.html\n",
    "    q_model_config = {\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "    policy_model_config = {\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"custom_model\": None,  # Use this to define a custom policy model.\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "    train_batch_size_per_learner = 8192,\n",
    "    num_steps_sampled_before_learning_starts = 6000,\n",
    "\n",
    ")\n",
    "config.evaluation(evaluation_interval=100)\n",
    "\n",
    "# # See model catalog for more options.\n",
    "# # https://docs.ray.io/en/latest/rllib/rllib-models.html\n",
    "# # config.model[\"fcnet_hiddens\"] = [512, 512, 512, 512, 512]\n",
    "# config.model[\"uses_new_env_runners\"] = True\n",
    "# config.model[\"fcnet_hiddens\"] = [1024, 1024, 1024, 1024, 1024]\n",
    "# config.model[\"use_lstm\"] = True\n",
    "# # config.model[\"lstm_cell_size\"] = 2048\n",
    "# config.model[\"lstm_cell_size\"] = 4096\n",
    "# config.model[\"max_seq_len\"] = 200\n",
    "# config.model[\"lstm_use_prev_action\"] = True\n",
    "\n",
    "algo = config.build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹시 이전 학습 결과를 로드할 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = Algorithm.from_checkpoint(\"./SAC_layer_512_5_32_slithering_low_healthy_reward2_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 네트워크 가중치 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = algo.get_weights()\n",
    "algo.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 교환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.set_weights(trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 파라미터 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.get_config().training().num_sgd_iter\n",
    "\n",
    "#Env runner 파라미터 보기.\n",
    "# algo.env_runner.config[\"exploration_config\"]\n",
    "# algo.get_config().model\n",
    "\n",
    "# algo.compute_single_action()\n",
    "algo.get_policy().model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "n_iter = 27000\n",
    "save_iter = 0\n",
    "save_name = \"SAC_layer_512_5_32_helix_sidecost\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(f\"{i:03d}th iteration done\")\n",
    "    # result.pop(\"config\")\n",
    "    # pprint(result)\n",
    "\n",
    "    if i%3000 == 0:\n",
    "        checkpoint_dir = algo.save(save_name+\"_\"+str(save_iter))\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        save_iter += 1\n",
    "\n",
    "algo.save(save_name+str(\"_final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경에서 학습된 Policy 테스트하기 (RL Module 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module import RLModule\n",
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "import time\n",
    "\n",
    "algo = Algorithm.from_checkpoint(\"./SAC_layer_512_5_32_helix2_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"horcrux_terrain_v1/plane-v1\", \n",
    "#                terminate_when_unhealthy = False, \n",
    "#                render_mode = \"human\", \n",
    "#             #    render_camera_name = 'ceiling', \n",
    "#                use_gait = True,\n",
    "#                gait_params = (30,30,40,40,0),\n",
    "#                **env_config,\n",
    "#                ) \n",
    "\n",
    "# obs, info = env.reset()\n",
    "\n",
    "# algo.get_policy().action_connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mediapy as media\n",
    "\n",
    "# env = gym.make(\"horcrux_terrain_v1/sand-v1\", \n",
    "#                terminate_when_unhealthy = False, \n",
    "#                render_mode = \"human\", \n",
    "#             #    render_camera_name = 'ceiling', \n",
    "#                use_gait = True,\n",
    "#                gait_params = (30,30,40,40,0),\n",
    "#                **env_config,\n",
    "#                ) \n",
    "# env_config['gait_params'] = (30,30,40,40,0)\n",
    "\n",
    "env = gym.make(\"horcrux_terrain_v1/climb-v1\", \n",
    "               terminate_when_unhealthy = False, \n",
    "               # render_mode = \"human\", \n",
    "               render_mode = \"rgb_array\", \n",
    "            #    render_camera_name = 'ceiling', \n",
    "               use_gait = True,\n",
    "               # gait_params = (30,30,40,40,0),\n",
    "               **env_config,\n",
    "               ) \n",
    "\n",
    "step_starting_index = 0\n",
    "episode_index = 1\n",
    "video_prefix = \"SAC_Slithering_\"\n",
    "\n",
    "frames = []\n",
    "\n",
    "for j in range(1):\n",
    "   episode_return = 0\n",
    "   terminated = truncated = False\n",
    "\n",
    "   obs, info = env.reset()\n",
    "\n",
    "   t_now = time.time()\n",
    "   for i in range(1000):\n",
    "      # while (time.time() - t_now) < 0.1:\n",
    "      #     pass\n",
    "      t_now = time.time()\n",
    "      action = algo.compute_single_action(obs, explore=False)\n",
    "      \n",
    "      obs, reward, terminated, truncated, info = env.step(action)\n",
    "      \n",
    "      pixels = env.render()\n",
    "      frames.append(pixels)\n",
    "\n",
    "      prev_a = action\n",
    "\n",
    "      if terminated:\n",
    "         print(\"terminated\")\n",
    "\n",
    "      episode_return += reward\n",
    "\n",
    "      # print(info['com_ypr'])\n",
    "      # print(f\"{info['x_velocity']} || {info['y_velocity']}\")\n",
    "\n",
    "   print(f\"Reached episode return of {episode_return}.\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "media.show_video(frames, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경에서 학습된 Policy 테스트하기 (PPO 알고리즘 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import time\n",
    "\n",
    "env = gym.make(\"horcrux_terrain_v1/plane-v1\", \n",
    "               terminate_when_unhealthy = False, \n",
    "               render_mode = \"human\", \n",
    "            #    render_camera_name = 'ceiling', \n",
    "               use_gait = True,\n",
    "               gait_params = (30,30,40,40,0),\n",
    "               **env_config,\n",
    "               ) \n",
    "\n",
    "for j in range(10):\n",
    "   episode_return = 0\n",
    "   terminated = truncated = False\n",
    "\n",
    "   obs, info = env.reset()\n",
    "\n",
    "   init_state = state = [np.zeros([4096], np.float32) for _ in range(200)]\n",
    "   prev_action = np.zeros((14), np.float32)\n",
    "   for i in range(1000):\n",
    "\n",
    "      a, init_state = algo.compute_single_action(observation= obs, state=init_state, prev_action=prev_action,policy_id=\"default_policy\")\n",
    "      \n",
    "      obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      prev_action = a\n",
    "      if terminated:\n",
    "         print(\"terminated\")\n",
    "\n",
    "      episode_return += reward\n",
    "\n",
    "   print(f\"Reached episode return of {episode_return}.\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdtor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
