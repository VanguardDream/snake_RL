{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ray RLlib 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요 패키지 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "from horcrux_terrain_v1.envs import PlaneWorld\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.tune.registry import register_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray 실행 (Warning 관련 무시 키워드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(runtime_env={\"env_vars\": {\"PYTHONWARNINGS\": \"ignore::DeprecationWarning\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym -> Rllib Env 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"forward_reward_weight\": 4,\n",
    "    \"side_cost_weight\": 2,\n",
    "    \"unhealthy_max_steps\": 75,\n",
    "    \"healthy_roll_range\": (-45,45),\n",
    "    \"terminating_roll_range\": (-85,85),\n",
    "    \"rotation_norm_cost_weight\": 0.05,\n",
    "    \"termination_reward\": 0,\n",
    "}\n",
    "\n",
    "# Sand\n",
    "register_env(\"sand-v1\", lambda config: SandWorld(forward_reward_weight=env_config[\"forward_reward_weight\"], \n",
    "                                                 side_cost_weight=env_config[\"side_cost_weight\"], \n",
    "                                                 unhealthy_max_steps=env_config[\"unhealthy_max_steps\"], \n",
    "                                                 healthy_roll_range=env_config[\"healthy_roll_range\"],\n",
    "                                                 terminating_roll_range=env_config[\"terminating_roll_range\"],\n",
    "                                                 rotation_norm_cost_weight=env_config[\"rotation_norm_cost_weight\"],\n",
    "                                                 termination_reward=env_config[\"termination_reward\"]))\n",
    "\n",
    "# Plane\n",
    "register_env(\"plane-v1\", lambda config: PlaneWorld(forward_reward_weight=env_config[\"forward_reward_weight\"], \n",
    "                                                 side_cost_weight=env_config[\"side_cost_weight\"], \n",
    "                                                 unhealthy_max_steps=env_config[\"unhealthy_max_steps\"], \n",
    "                                                 healthy_roll_range=env_config[\"healthy_roll_range\"],\n",
    "                                                 terminating_roll_range=env_config[\"terminating_roll_range\"],\n",
    "                                                 rotation_norm_cost_weight=env_config[\"rotation_norm_cost_weight\"],\n",
    "                                                 termination_reward=env_config[\"termination_reward\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 알고리즘 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig()\n",
    "# Activate new API stack. -> 구려서 안씀.\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=False,\n",
    "    enable_env_runner_and_connector_v2=False,\n",
    ")\n",
    "# config.environment(\"sand-v1\")\n",
    "config.environment(\"plane-v1\")\n",
    "config.framework(\"torch\")\n",
    "total_workers = 12\n",
    "config.resources(num_gpus=1,num_cpus_per_worker=1, num_gpus_per_worker= 1/(total_workers+1))\n",
    "config.rollouts(num_rollout_workers=total_workers)\n",
    "config.training(\n",
    "    gamma=0.9, \n",
    "    lr=0.001, \n",
    "    # kl_coeff=0.3, \n",
    "\n",
    "    # See model catalog for more options.\n",
    "    # https://docs.ray.io/en/latest/rllib/rllib-models.html\n",
    "    model={ \"fcnet_hiddens\": [512, 512, 512, 512, 512],\n",
    "            },\n",
    ")\n",
    "config.evaluation(evaluation_interval=100)\n",
    "\n",
    "# # See model catalog for more options.\n",
    "# # https://docs.ray.io/en/latest/rllib/rllib-models.html\n",
    "# # config.model[\"fcnet_hiddens\"] = [512, 512, 512, 512, 512]\n",
    "# config.model[\"uses_new_env_runners\"] = True\n",
    "# config.model[\"fcnet_hiddens\"] = [1024, 1024, 1024, 1024, 1024]\n",
    "# config.model[\"use_lstm\"] = True\n",
    "# # config.model[\"lstm_cell_size\"] = 2048\n",
    "# config.model[\"lstm_cell_size\"] = 4096\n",
    "# config.model[\"max_seq_len\"] = 200\n",
    "# config.model[\"lstm_use_prev_action\"] = True\n",
    "\n",
    "algo = config.build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹시 이전 학습 결과를 로드할 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo = Algorithm.from_checkpoint(\"./agents/ModelV2_512x5_2048_v0924_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 파라미터 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.get_config().training().num_sgd_iter\n",
    "\n",
    "#Env runner 파라미터 보기.\n",
    "# algo.env_runner.config[\"exploration_config\"]\n",
    "# algo.get_config().model\n",
    "\n",
    "# algo.compute_single_action()\n",
    "# algo.get_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "n_iter = 1000\n",
    "save_iter = 0\n",
    "save_name = \"ModelV2_512x5\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(f\"{i:03d}th iteration done\")\n",
    "    # result.pop(\"config\")\n",
    "    # pprint(result)\n",
    "\n",
    "    if i%60 == 0:\n",
    "        checkpoint_dir = algo.save(save_name+\"_\"+str(save_iter))\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        save_iter += 1\n",
    "\n",
    "algo.save(save_name+str(\"_final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경에서 학습된 Policy 테스트하기 (RL Module 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module import RLModule\n",
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "import time\n",
    "\n",
    "algo = Algorithm.from_checkpoint(\"./ModelV2_512x5_11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"horcrux_terrain_v1/plane-v1\", \n",
    "#                terminate_when_unhealthy = False, \n",
    "#                render_mode = \"human\", \n",
    "#             #    render_camera_name = 'ceiling', \n",
    "#                use_gait = True,\n",
    "#                gait_params = (30,30,40,40,0),\n",
    "#                **env_config,\n",
    "#                ) \n",
    "\n",
    "# obs, info = env.reset()\n",
    "\n",
    "# algo.get_policy().action_connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"horcrux_terrain_v1/sand-v1\", \n",
    "#                terminate_when_unhealthy = False, \n",
    "#                render_mode = \"human\", \n",
    "#             #    render_camera_name = 'ceiling', \n",
    "#                use_gait = True,\n",
    "#                gait_params = (30,30,40,40,0),\n",
    "#                **env_config,\n",
    "#                ) \n",
    "\n",
    "env = gym.make(\"horcrux_terrain_v1/plane-v1\", \n",
    "               terminate_when_unhealthy = False, \n",
    "               render_mode = \"human\", \n",
    "            #    render_camera_name = 'ceiling', \n",
    "               use_gait = True,\n",
    "               gait_params = (30,30,40,40,0),\n",
    "               **env_config,\n",
    "               ) \n",
    "\n",
    "for j in range(5):\n",
    "   episode_return = 0\n",
    "   terminated = truncated = False\n",
    "\n",
    "   obs, info = env.reset()\n",
    "\n",
    "\n",
    "   for i in range(6000):\n",
    "      action = algo.compute_single_action(obs, explore=False)\n",
    "      \n",
    "      obs, reward, terminated, truncated, info = env.step(action)\n",
    "      \n",
    "      prev_a = action\n",
    "\n",
    "      if terminated:\n",
    "         print(\"terminated\")\n",
    "\n",
    "      episode_return += reward\n",
    "\n",
    "   print(f\"Reached episode return of {episode_return}.\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경에서 학습된 Policy 테스트하기 (PPO 알고리즘 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import time\n",
    "\n",
    "env = gym.make(\"horcrux_terrain_v1/plane-v1\", \n",
    "               terminate_when_unhealthy = False, \n",
    "               render_mode = \"human\", \n",
    "            #    render_camera_name = 'ceiling', \n",
    "               use_gait = True,\n",
    "               gait_params = (30,30,40,40,0),\n",
    "               **env_config,\n",
    "               ) \n",
    "\n",
    "for j in range(10):\n",
    "   episode_return = 0\n",
    "   terminated = truncated = False\n",
    "\n",
    "   obs, info = env.reset()\n",
    "\n",
    "   init_state = state = [np.zeros([4096], np.float32) for _ in range(200)]\n",
    "   prev_action = np.zeros((14), np.float32)\n",
    "   for i in range(1000):\n",
    "\n",
    "      a, init_state = algo.compute_single_action(observation= obs, state=init_state, prev_action=prev_action,policy_id=\"default_policy\")\n",
    "      \n",
    "      obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      prev_action = a\n",
    "      if terminated:\n",
    "         print(\"terminated\")\n",
    "\n",
    "      episode_return += reward\n",
    "\n",
    "   print(f\"Reached episode return of {episode_return}.\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdtor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
