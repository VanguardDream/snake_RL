{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada3dd6c-e196-4236-87e2-59bd12ee93c9",
   "metadata": {},
   "source": [
    "# Horcrux Joystick 입력 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d86890-4e2f-4c3e-a30c-b281c5de415b",
   "metadata": {},
   "source": [
    "## 필요 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f4abf-b333-49c3-9ea6-22f049e2eb51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 조이스틱 환경 삽입\n",
    "import horcrux_terrain_v2\n",
    "# from horcrux_terrain_v2.envs import PlaneJoyWorld\n",
    "from horcrux_terrain_v2.envs import PlaneJoyDirWorld\n",
    "\n",
    "# Ray 패키지 삽입\n",
    "import ray\n",
    "import os\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import mediapy as media\n",
    "\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93044ef8",
   "metadata": {},
   "source": [
    "# 사용자 구성 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe50062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "class CustomSACModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        model_shape = model_config['fcnet_hiddens']\n",
    "        print(model_config)\n",
    "\n",
    "        # Shared actor trunk\n",
    "        self.shared = FullyConnectedNetwork(\n",
    "            obs_space, action_space, model_shape[-1], model_config, name + \"_shared\"\n",
    "        )\n",
    "\n",
    "        # Value network head 확장\n",
    "        self.value_branch = nn.Sequential(\n",
    "            nn.Linear(model_shape[-1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self._value_out = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        features, _ = self.shared(input_dict, state, seq_lens)\n",
    "        self._value_out = self.value_branch(features)\n",
    "        return features, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self._value_out.squeeze(1)\n",
    "    \n",
    "    \n",
    "ModelCatalog.register_custom_model(\"custom_sac_model\", CustomSACModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf2442",
   "metadata": {},
   "source": [
    "# 필요 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_filename(base_path, ext=\".mp4\"):\n",
    "    \"\"\"중복된 파일명이 존재하면 숫자를 증가하여 새로운 경로를 반환\"\"\"\n",
    "    if not base_path.endswith(ext):\n",
    "        base_path += ext  # 확장자 자동 추가\n",
    "\n",
    "    file_name, file_ext = os.path.splitext(base_path)  # 파일명과 확장자 분리\n",
    "    count = 0\n",
    "    new_path = f\"{file_name}-episode-0\"+file_ext\n",
    "\n",
    "    while os.path.exists(new_path):  # 파일 존재 여부 확인\n",
    "        new_path = f\"{file_name}{count}-episode-0{file_ext}\"\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    return f\"rl-video{count-1}\", new_path\n",
    "\n",
    "\n",
    "def default_plot(x, y, f_name='default_plot', legends=['acc_x', 'acc_y', 'acc_z'], title=''):\n",
    "    colors = plt.get_cmap(\"tab10\").colors\n",
    "    fig, ax = plt.subplots(figsize=(15/2.54, 10/2.54))\n",
    "    ax.set_facecolor((0.95, 0.95, 0.95)) \n",
    "\n",
    "    n_column = len(np.shape(y))\n",
    "    if n_column>2:\n",
    "        print(\"The dimmension of data must be less than 3. (1D or 2D)\")\n",
    "        return -1\n",
    "    \n",
    "    n_data = np.shape(y)[1]\n",
    "\n",
    "    for i in range(n_data):\n",
    "        # **Plot**\n",
    "        ax.plot(x, y[:,i], linewidth=1.5, linestyle=\"-\", color=colors[i], label=legends[i])\n",
    "        # ax.plot(x, y[:,i], linewidth=1.5, linestyle=\"-\", color=colors[1], label=legends[1])\n",
    "        # ax.plot(x, y[:,i], linewidth=1.5, linestyle=\"-\", color=colors[2], label=legends[2])\n",
    "\n",
    "    # **Grid 설정**\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=1, color=\"#202020\", alpha=0.7)  # 주요 그리드\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(True, which=\"minor\", linestyle=\":\", linewidth=0.5, color=\"#404040\", alpha=0.5)  # 보조 그리드\n",
    "\n",
    "    # **Axis 스타일 설정**\n",
    "    ax.spines[\"top\"].set_linewidth(1.0)\n",
    "    ax.spines[\"right\"].set_linewidth(1.0)\n",
    "    ax.spines[\"left\"].set_linewidth(1.0)\n",
    "    ax.spines[\"bottom\"].set_linewidth(1.0)\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=11, width=1.0)  # 폰트 크기 및 라인 두께\n",
    "    ax.xaxis.label.set_size(12)\n",
    "    ax.yaxis.label.set_size(12)\n",
    "\n",
    "    # **폰트 및 제목 설정**\n",
    "    plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "    ax.set_xlabel(\"X-Axis\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Y-Axis\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # **Legend (MATLAB 스타일 적용)**\n",
    "    ax.legend(loc=\"upper right\", ncol=3, fontsize=10, frameon=True)\n",
    "\n",
    "    # **비율 설정 (MATLAB의 `pbaspect([2.1 1 1])`과 비슷한 효과)**\n",
    "    fig.set_size_inches(2.1 * 5, 5)  # 비율 2.1:1 (기본 높이 5inch 기준)\n",
    "\n",
    "    # **Save Figure (MATLAB saveas와 유사)**\n",
    "    plt.savefig(f\"./figs/{f_name}.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, kernel, mode='same')  # 'valid'는 경계 제외\n",
    "\n",
    "\n",
    "def get_data_from_info(info):\n",
    "    # Action info\n",
    "    action = np.array([_info['action'] for _info in info])\n",
    "\n",
    "    # Status info\n",
    "    stat_init_rpy = np.array([_info['init_rpy'] for _info in info])\n",
    "    stat_init_com = np.array([_info['init_com'] for _info in info])\n",
    "    stat_xy_vel = np.array([[_info['x_velocity'], _info['y_velocity']] for _info in info])\n",
    "    stat_yaw_vel = np.array([_info['yaw_velocity'] for _info in info])\n",
    "    stat_quat = np.array([_info['head_quat'] for _info in info])\n",
    "    stat_ang_vel = np.array([_info['head_ang_vel'] for _info in info])\n",
    "    stat_lin_acc = np.array([_info['head_lin_acc'] for _info in info])\n",
    "    stat_motion_vector = np.array([_info['motion_vector'] for _info in info])\n",
    "    stat_com_pos = np.array([_info['com_pos'] for _info in info])\n",
    "    stat_com_ypr = np.array([_info['com_ypr'] for _info in info])\n",
    "    stat_step_ypr = np.array([_info['step_ypr'] for _info in info])\n",
    "    stat_reward_func_orientation = np.array([_info['reward_func_orientation'] for _info in info])\n",
    "    \n",
    "\n",
    "    # Rew info\n",
    "    rew_linear_movement = np.array([_info['reward_linear_movement'] for _info in info])\n",
    "    reward_angular_movement = np.array([_info['reward_angular_movement'] for _info in info])\n",
    "    reward_efficiency = np.array([_info['reward_efficiency'] for _info in info])\n",
    "    reward_healthy = np.array([_info['reward_healthy'] for _info in info])\n",
    "    cost_ctrl = np.array([_info['cost_ctrl'] for _info in info])\n",
    "    cost_unhealthy = np.array([_info['cost_unhealthy'] for _info in info])\n",
    "    cost_orientation = np.array([_info['cost_orientation'] for _info in info])\n",
    "    cost_yaw_vel = np.array([_info['cost_yaw_vel'] for _info in info])\n",
    "    direction_similarity = np.array([_info['direction_similarity'] for _info in info])\n",
    "    rotation_alignment = np.array([_info['rotation_alignment'] for _info in info])\n",
    "    vel_orientation = np.array([_info['velocity_theta'] for _info in info])\n",
    "\n",
    "    # Input info\n",
    "    input_joy = np.array([_info['joy_input'] for _info in info])\n",
    "    gait_param = np.array([_info['gait_params'] for _info in info])\n",
    "\n",
    "    data_dict = {\n",
    "        'action': action,\n",
    "        'stat_init_rpy': stat_init_rpy,\n",
    "        'stat_init_com': stat_init_com,\n",
    "        'stat_xy_vel': stat_xy_vel,\n",
    "        'stat_yaw_vel': stat_yaw_vel,\n",
    "        'stat_quat': stat_quat,\n",
    "        'stat_ang_vel': stat_ang_vel,\n",
    "        'stat_lin_acc': stat_lin_acc,\n",
    "        'stat_motion_vector': stat_motion_vector,\n",
    "        'stat_com_pos': stat_com_pos,\n",
    "        'stat_com_ypr': stat_com_ypr,\n",
    "        'stat_com_r_ypr':stat_reward_func_orientation,\n",
    "        'stat_step_ypr': stat_step_ypr,\n",
    "\n",
    "        'rew_linear_movement': rew_linear_movement,\n",
    "        'reward_angular_movement': reward_angular_movement,\n",
    "        'reward_efficiency': reward_efficiency,\n",
    "        'reward_healthy': reward_healthy,\n",
    "        'cost_ctrl': cost_ctrl,\n",
    "        'cost_unhealthy': cost_unhealthy,\n",
    "        'cost_orientation': cost_orientation,\n",
    "        'cost_yaw_vel': cost_yaw_vel,\n",
    "        'direction_similarity': direction_similarity,\n",
    "        'rotation_alignment': rotation_alignment,\n",
    "        'vel_orientation': vel_orientation,\n",
    "\n",
    "        'input_joy': input_joy,\n",
    "        'gait_param': gait_param,\n",
    "    }\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517ec57-16da-4789-a76f-2c77998e7a5e",
   "metadata": {},
   "source": [
    "## Ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31cbbe-264f-4298-a2f1-471cf823d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(dashboard_host=\"0.0.0.0\", dashboard_port=8265)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39407944-23a9-42b0-854b-c19f1c43bcdc",
   "metadata": {},
   "source": [
    "## Gym 환경 등록하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809e905-5daf-45c4-919f-98e53d97572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"forward_reward_weight\": 100.0,\n",
    "    \"rotation_reward_weight\": 100.0,\n",
    "    \"unhealthy_max_steps\": 150.0,\n",
    "    \"healthy_reward\": 3.0,\n",
    "    \"healthy_roll_range\": (-40,40),\n",
    "    \"terminating_roll_range\": (-85,85),\n",
    "    \"rotation_norm_cost_weight\": 8.5,\n",
    "    \"termination_reward\": 0,\n",
    "    \"gait_params\": (30, 30, 40, 40, 45),\n",
    "    \"use_friction_chg\": True,\n",
    "    \"joy_input_random\": True,\n",
    "    \"use_imu_window\": True,\n",
    "    \"use_vels_window\": True,\n",
    "    \"ctrl_cost_weight\": 0.05,\n",
    "}\n",
    "\n",
    "render_env_config = env_config.copy()\n",
    "render_env_config['render_mode'] = 'rgb_array'\n",
    "render_env_config['render_camera_name'] = 'ceiling'\n",
    "\n",
    "# env = gym.make(\"horcrux_terrain_v2/plane-v2\", **render_env_config)\n",
    "\n",
    "# JoyWorld\n",
    "register_env(\"joy-v1\", lambda config: PlaneJoyDirWorld( forward_reward_weight=env_config[\"forward_reward_weight\"], \n",
    "                                                     rotation_reward_weight=env_config[\"rotation_reward_weight\"], \n",
    "                                                     unhealthy_max_steps=env_config[\"unhealthy_max_steps\"],\n",
    "                                                     healthy_reward=env_config[\"healthy_reward\"], \n",
    "                                                     healthy_roll_range=env_config[\"healthy_roll_range\"],\n",
    "                                                     terminating_roll_range=env_config[\"terminating_roll_range\"],\n",
    "                                                     rotation_norm_cost_weight=env_config[\"rotation_norm_cost_weight\"],\n",
    "                                                     termination_reward=env_config[\"termination_reward\"],\n",
    "                                                     gait_params=env_config[\"gait_params\"],\n",
    "                                                     use_friction_chg=env_config[\"use_friction_chg\"],\n",
    "                                                     joy_input_random=env_config[\"joy_input_random\"],\n",
    "                                                     use_imu_window=env_config[\"use_imu_window\"],\n",
    "                                                     ctrl_cost_weight=env_config[\"ctrl_cost_weight\"],\n",
    "                                                   )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf2134-3167-46f1-b28b-7d305065f559",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 설정하기 PPO\n",
    "+ 신형 API 구조 사용해보기 (get_policy() 메서드 오류 생김... 추론 못함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436edebc-bcaa-45fa-9941-58ad2868655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.rllib.core.rl_module import RLModuleSpec\n",
    "# from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n",
    "#     PPOTorchRLModule\n",
    "# )\n",
    "\n",
    "# from ray.rllib.examples.rl_modules.classes.lstm_containing_rlm import (\n",
    "#     LSTMContainingRLModule,\n",
    "# )\n",
    "# from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "\n",
    "# config = PPOConfig()\n",
    "\n",
    "# # 구형 API 구조 사용\n",
    "# config.api_stack(\n",
    "#     enable_rl_module_and_learner=True,\n",
    "#     enable_env_runner_and_connector_v2=True,\n",
    "# )\n",
    "\n",
    "# config.environment(\"joy-v1\")\n",
    "# config.framework(\"torch\")\n",
    "\n",
    "# # 병렬 CPU 사용 설정\n",
    "# total_workers = 16\n",
    "# config.learners(num_learners = 1, num_gpus_per_learner=1)\n",
    "# config.env_runners(num_env_runners = total_workers, num_cpus_per_env_runner = 1, rollout_fragment_length = 'auto')\n",
    "# config.rl_module(\n",
    "#     rl_module_spec=RLModuleSpec(\n",
    "#         module_class=LSTMContainingRLModule,\n",
    "#         learner_only=False,\n",
    "#         inference_only=False,\n",
    "#     ),\n",
    "#     model_config = DefaultModelConfig(\n",
    "#         fcnet_hiddens=[512, 512, 512, 512, 512, 512],\n",
    "#         fcnet_activation=\"swish\",\n",
    "#         use_lstm=True,\n",
    "#         max_seq_len=100,\n",
    "#         lstm_use_prev_action=True,\n",
    "#         lstm_cell_size=256,\n",
    "#     ),\n",
    "# )\n",
    "# config.training(\n",
    "#     # Default config sets\n",
    "#     gamma=0.95,\n",
    "#     lr=0.0001,\n",
    "#     train_batch_size_per_learner = 100000,\n",
    "#     minibatch_size = 5000,\n",
    "#     num_epochs = 10,\n",
    "#     shuffle_batch_per_epoch = False,\n",
    "\n",
    "#     # PPO config sets\n",
    "#     entropy_coeff = 0.01,\n",
    "#     vf_loss_coeff = 0.5, #이 값 튜닝 진행해야함. (기본값 : 1.0)\n",
    "#     vf_clip_param = 5,\n",
    "# )\n",
    "\n",
    "# algo = config.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d102417",
   "metadata": {},
   "source": [
    "+ 구형 API 사용해서 구현\n",
    "RNN 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "config = PPOConfig()\n",
    "\n",
    "# 구형 API 구조 사용\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=False,\n",
    "    enable_env_runner_and_connector_v2=False,\n",
    ")\n",
    "\n",
    "config.environment(\"joy-v1\")\n",
    "config.framework(\"torch\")\n",
    "config.resources(\n",
    "    num_cpus_for_main_process=4,\n",
    "    num_gpus=1,\n",
    "    num_gpus_per_learner_worker=1,\n",
    ")\n",
    "\n",
    "# 병렬 CPU 사용 설정\n",
    "total_workers = 16\n",
    "config.learners(num_learners = 1, num_gpus_per_learner=1)\n",
    "config.env_runners(num_env_runners = total_workers, num_cpus_per_env_runner = 1, rollout_fragment_length = 'auto')\n",
    "\n",
    "config.training(\n",
    "    # Default config sets\n",
    "    gamma=0.95,\n",
    "    lr=0.0005,\n",
    "    train_batch_size = 100000,\n",
    "    minibatch_size = 10000,\n",
    "    num_epochs = 40,\n",
    "    shuffle_batch_per_epoch = True,\n",
    "    model = {\n",
    "        # \"fcnet_hiddens\": [256, 256, 256, 256, 64],\n",
    "        \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "        \"fcnet_activation\": \"tanh\",\n",
    "        # \"post_fcnet_hiddens\": [],\n",
    "        # \"post_fcnet_activation\": \"tanh\",\n",
    "        'vf_share_layers': False, #원래는 False로 학습했었음.\n",
    "        \"use_lstm\": False,\n",
    "        # \"max_seq_len\": 40,\n",
    "        # \"lstm_use_prev_action\": True,\n",
    "        # \"lstm_cell_size\": 64,\n",
    "\n",
    "        # \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "        # \"custom_model_config\": {},\n",
    "    },\n",
    "\n",
    "    # PPO config sets\n",
    "    clip_param=0.25,                # 기본값 0.3\n",
    "    entropy_coeff = 0.03,          # 기본값 0.01\n",
    "    kl_coeff=0.3,                  # 기본값 0.2\n",
    "    lambda_=1.0,                   # 기본값 1.0\n",
    "    vf_loss_coeff = 0.7,           # 이 값 튜닝 진행해야함. (기본값 : 1.0)\n",
    "    vf_clip_param = 5.0,            # 기본값 5\n",
    "    grad_clip=0.5,                # 기본값 0.5\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "try:\n",
    "    if prior_weight:\n",
    "        algo.set_weights(prior_weight)\n",
    "        print(\"Prior weight is set to loaded weight.\")\n",
    "\n",
    "except:\n",
    "    print(\"Prior weight does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.get_policy().model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe669f",
   "metadata": {},
   "source": [
    "+ RNN 미사용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beae10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig()\n",
    "\n",
    "# 구형 API 구조 사용\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=False,\n",
    "    enable_env_runner_and_connector_v2=False,\n",
    ")\n",
    "\n",
    "config.environment(\"joy-v1\")\n",
    "config.framework(\"torch\")\n",
    "config.resources(\n",
    "    num_cpus_for_main_process=8,\n",
    "    num_gpus=1,\n",
    "    # num_learner_workers=1,\n",
    "    # num_gpus_per_learner_worker=1,\n",
    ")\n",
    "\n",
    "config.learners(\n",
    "    num_learners=0,\n",
    "    num_gpus_per_learner=1,\n",
    ")\n",
    "\n",
    "# 병렬 CPU 사용 설정\n",
    "total_workers = 16\n",
    "config.env_runners(num_env_runners = total_workers, num_cpus_per_env_runner = 1, rollout_fragment_length = 'auto')\n",
    "\n",
    "config.training(\n",
    "    # Default config sets\n",
    "    gamma=0.95,\n",
    "    lr=0.0005,\n",
    "    train_batch_size = 100000,\n",
    "    minibatch_size = 10000,\n",
    "    num_epochs = 10,\n",
    "    shuffle_batch_per_epoch = False,\n",
    "    model = {\n",
    "        \"fcnet_hiddens\": [512, 512, 512, 512, 512, 512],\n",
    "        \"fcnet_activation\": \"swish\",\n",
    "        # \"post_fcnet_hiddens\": [],\n",
    "        # \"post_fcnet_activation\": \"tanh\",\n",
    "\n",
    "        # \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "        # \"custom_model_config\": {},\n",
    "    },\n",
    "\n",
    "    # PPO config sets\n",
    "    entropy_coeff = 0.01,\n",
    "    vf_loss_coeff = 0.5, #이 값 튜닝 진행해야함. (기본값 : 1.0)\n",
    "    vf_clip_param = 5,\n",
    ")\n",
    "\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba781dc0",
   "metadata": {},
   "source": [
    "+ 학습된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/bong/Project/snake_RL/GD_tor/learning/ray239-ppo'\n",
    "\n",
    "## LP Final\n",
    "# GD_tor/learning/ray239-ppo/learned_policy/20250418-LP-Final/PPO_NEW_512_7_416_vf_noshare_38\n",
    "## LP Best\n",
    "# GD_tor/learning/ray239-ppo/learned_policy/20250418-LP-Final/PPO_NEW_512_7_416_vf_noshare_2_best\n",
    "\n",
    "algo = Algorithm.from_checkpoint(path=base_path + '/learned_policy/20250418-LP-Final/PPO_NEW_512_7_416_vf_noshare_2_best/')\n",
    "\n",
    "# config = algo.get_config()\n",
    "# prior_weight = algo.get_weights()\n",
    "# algo.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefac126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "\n",
    "# pprint(DefaultModelConfig())\n",
    "# pprint(algo.get_config().to_dict())\n",
    "# algo.get_default_policy_class(config).\n",
    "# algo.get_module()\n",
    "\n",
    "# algo.get_config().to_dict()\n",
    "# algo.get_module().get_initial_state()\n",
    "# algo.get_metadata()\n",
    "algo.get_policy().model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae756f-bb08-4813-bf07-c019392dac99",
   "metadata": {},
   "source": [
    "## 학습시작\n",
    "+ 구형 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae0dfe-e461-442b-b090-cc437723543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import datetime\n",
    "from scipy.io import savemat\n",
    "\n",
    "n_iter = 1000\n",
    "save_iter = 0\n",
    "save_name = \"PPO_8DIR_512_5_421\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(f\"{i:03d}\", end=\", \")\n",
    "    # result.pop(\"config\")\n",
    "    # pprint(result)\n",
    "\n",
    "    if i%50 == 0:\n",
    "        checkpoint_dir = algo.save(save_name + \"_\" + str(save_iter))\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        save_iter += 1\n",
    "\n",
    "\n",
    "        # Record Validation Env\n",
    "        env = gym.make(\"horcrux_terrain_v2/plane-v3\", **render_env_config)\n",
    "        obs = env.reset()[0]\n",
    "        env_done = False\n",
    "        init_prev_a = prev_a = np.array([0]*14)\n",
    "        lstm_cell_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "\n",
    "        if algo.config.enable_rl_module_and_learner:\n",
    "            init_state = state = algo.get_policy().model.get_initial_state()\n",
    "        else:\n",
    "            init_state = state = [np.zeros([lstm_cell_size], np.float32) for _ in range(2)]\n",
    "\n",
    "        rew_return = 0\n",
    "        frames = []\n",
    "        info = []\n",
    "\n",
    "        for i in range(3000):\n",
    "            act, _state_out, _ = algo.compute_single_action(observation=obs, state=state, prev_action=prev_a, explore=False)\n",
    "            obs, _step_rew, _, env_done, env_info = env.step(act)\n",
    "            pixels = env.render()\n",
    "            frames.append(pixels)\n",
    "            info.append(env_info)\n",
    "            rew_return += _step_rew\n",
    "            state = _state_out\n",
    "            prev_a = act\n",
    "\n",
    "        _video_base_name = 'rl-video'\n",
    "\n",
    "        _f_name, _full_path = get_unique_filename(f\"./video/{_video_base_name}\")\n",
    "        rew_dict = get_data_from_info(info)\n",
    "        rew_dict['rew_return'] = rew_return\n",
    "\n",
    "        # Save Video\n",
    "        save_video(frames, \"./video/\", name_prefix=_f_name, fps=env.metadata['render_fps'])\n",
    "\n",
    "        # Save Video Info\n",
    "        _f_video_info = open(f\"./video/joy_input.txt\", 'a')\n",
    "        _f_video_info.write(f'File creation time: {datetime.datetime.now()}\\n')\n",
    "        _f_video_info.write(f'Video file name: {_f_name}, Joy input: {info[0][\"joy_input\"]}, Friction: {info[0][\"friction_coeff\"]}\\n')\n",
    "        _f_video_info.close()\n",
    "\n",
    "        # Save Reward Info mat file\n",
    "        savemat(f\"./data/{save_name}_{_f_name}.mat\", rew_dict)\n",
    "\n",
    "        env.reset()\n",
    "        env.close()\n",
    "\n",
    "\n",
    "algo.save(save_name + str(\"_final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7a38d",
   "metadata": {},
   "source": [
    "정책 녹화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import datetime\n",
    "from scipy.io import savemat\n",
    "\n",
    "config = algo.get_config()\n",
    "\n",
    "save_name = \"8DIR-1-0\"\n",
    "num_videos = 30\n",
    "for iteration in range(num_videos):\n",
    "    # Record Validation Env\n",
    "    env = gym.make(\"horcrux_terrain_v2/plane-v3\", **render_env_config)\n",
    "    obs = env.reset()[0]\n",
    "    env_done = False\n",
    "    init_prev_a = prev_a = np.array([0]*14)\n",
    "    lstm_cell_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "\n",
    "    if algo.config.enable_rl_module_and_learner:\n",
    "        init_state = state = algo.get_policy().model.get_initial_state()\n",
    "    else:\n",
    "        init_state = state = [np.zeros([lstm_cell_size], np.float32) for _ in range(2)]\n",
    "\n",
    "    rew_return = 0\n",
    "    frames = []\n",
    "    info = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        act, _state_out, _ = algo.compute_single_action(observation=obs, state=state, prev_action=prev_a)\n",
    "        obs, _step_rew, _, env_done, env_info = env.step(act)\n",
    "        pixels = env.render()\n",
    "        frames.append(pixels)\n",
    "        info.append(env_info)\n",
    "        rew_return += _step_rew\n",
    "        state = _state_out\n",
    "        prev_a = act\n",
    "\n",
    "    _video_base_name = 'rl-video'\n",
    "\n",
    "    _f_name, _full_path = get_unique_filename(f\"./video/{_video_base_name}\")\n",
    "    rew_dict = get_data_from_info(info)\n",
    "    rew_dict['rew_return'] = rew_return\n",
    "\n",
    "    # Save Video\n",
    "    save_video(frames, \"./video/\", name_prefix=_f_name, fps=env.metadata['render_fps'])\n",
    "\n",
    "    # Save Video Info\n",
    "    _f_video_info = open(f\"./video/joy_input.txt\", 'a')\n",
    "    _f_video_info.write(f'File creation time: {datetime.datetime.now()}\\n')\n",
    "    _f_video_info.write(f'Video file name: {_f_name}, Joy input: {info[0][\"joy_input\"]}, Friction: {info[0][\"friction_coeff\"]}\\n')\n",
    "    _f_video_info.close()\n",
    "\n",
    "    # Save Reward Info mat file\n",
    "    savemat(f\"./data/{save_name}_{_f_name}.mat\", rew_dict)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46573140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.get_module().input_specs_train\n",
    "# algo.get_module().input_specs_inference()\n",
    "\n",
    "\n",
    "algo.get_policy().model.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf445a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd239",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
