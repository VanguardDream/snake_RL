{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada3dd6c-e196-4236-87e2-59bd12ee93c9",
   "metadata": {},
   "source": [
    "# Horcrux Joystick 입력 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d86890-4e2f-4c3e-a30c-b281c5de415b",
   "metadata": {},
   "source": [
    "## 필요 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f4abf-b333-49c3-9ea6-22f049e2eb51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 조이스틱 환경 삽입\n",
    "import horcrux_terrain_v2\n",
    "# from horcrux_terrain_v2.envs import PlaneJoyWorld\n",
    "from horcrux_terrain_v2.envs import PlaneJoyDirWorld\n",
    "\n",
    "# Ray 패키지 삽입\n",
    "import ray\n",
    "import os\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import mediapy as media\n",
    "\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93044ef8",
   "metadata": {},
   "source": [
    "# 사용자 구성 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe50062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "class CustomSACModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        model_shape = model_config['fcnet_hiddens']\n",
    "        print(model_config)\n",
    "\n",
    "        # Shared actor trunk\n",
    "        self.shared = FullyConnectedNetwork(\n",
    "            obs_space, action_space, model_shape[-1], model_config, name + \"_shared\"\n",
    "        )\n",
    "\n",
    "        # Value network head 확장\n",
    "        self.value_branch = nn.Sequential(\n",
    "            nn.Linear(model_shape[-1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self._value_out = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        features, _ = self.shared(input_dict, state, seq_lens)\n",
    "        self._value_out = self.value_branch(features)\n",
    "        return features, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self._value_out.squeeze(1)\n",
    "    \n",
    "    \n",
    "ModelCatalog.register_custom_model(\"custom_sac_model\", CustomSACModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf2442",
   "metadata": {},
   "source": [
    "# 필요 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_filename(base_path, ext=\".mp4\"):\n",
    "    \"\"\"중복된 파일명이 존재하면 숫자를 증가하여 새로운 경로를 반환\"\"\"\n",
    "    if not base_path.endswith(ext):\n",
    "        base_path += ext  # 확장자 자동 추가\n",
    "\n",
    "    file_name, file_ext = os.path.splitext(base_path)  # 파일명과 확장자 분리\n",
    "    count = 0\n",
    "    new_path = f\"{file_name}-episode-0\"+file_ext\n",
    "\n",
    "    while os.path.exists(new_path):  # 파일 존재 여부 확인\n",
    "        new_path = f\"{file_name}{count}-episode-0{file_ext}\"\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    return f\"rl-video{count-1}\", new_path\n",
    "\n",
    "\n",
    "def default_plot(x, y, f_name='default_plot', legends=['acc_x', 'acc_y', 'acc_z'], title=''):\n",
    "    colors = plt.get_cmap(\"tab10\").colors\n",
    "    fig, ax = plt.subplots(figsize=(15/2.54, 10/2.54))\n",
    "    ax.set_facecolor((0.95, 0.95, 0.95)) \n",
    "\n",
    "    n_column = len(np.shape(y))\n",
    "    if n_column>2:\n",
    "        print(\"The dimmension of data must be less than 3. (1D or 2D)\")\n",
    "        return -1\n",
    "    \n",
    "    n_data = np.shape(y)[1]\n",
    "\n",
    "    for i in range(n_data):\n",
    "        # **Plot**\n",
    "        ax.plot(x, y[:,i], linewidth=1.5, linestyle=\"-\", color=colors[i], label=legends[i])\n",
    "        # ax.plot(x, y[:,i], linewidth=1.5, linestyle=\"-\", color=colors[1], label=legends[1])\n",
    "        # ax.plot(x, y[:,i], linewidth=1.5, linestyle=\"-\", color=colors[2], label=legends[2])\n",
    "\n",
    "    # **Grid 설정**\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=1, color=\"#202020\", alpha=0.7)  # 주요 그리드\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(True, which=\"minor\", linestyle=\":\", linewidth=0.5, color=\"#404040\", alpha=0.5)  # 보조 그리드\n",
    "\n",
    "    # **Axis 스타일 설정**\n",
    "    ax.spines[\"top\"].set_linewidth(1.0)\n",
    "    ax.spines[\"right\"].set_linewidth(1.0)\n",
    "    ax.spines[\"left\"].set_linewidth(1.0)\n",
    "    ax.spines[\"bottom\"].set_linewidth(1.0)\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=11, width=1.0)  # 폰트 크기 및 라인 두께\n",
    "    ax.xaxis.label.set_size(12)\n",
    "    ax.yaxis.label.set_size(12)\n",
    "\n",
    "    # **폰트 및 제목 설정**\n",
    "    plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "    ax.set_xlabel(\"X-Axis\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Y-Axis\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # **Legend (MATLAB 스타일 적용)**\n",
    "    ax.legend(loc=\"upper right\", ncol=3, fontsize=10, frameon=True)\n",
    "\n",
    "    # **비율 설정 (MATLAB의 `pbaspect([2.1 1 1])`과 비슷한 효과)**\n",
    "    fig.set_size_inches(2.1 * 5, 5)  # 비율 2.1:1 (기본 높이 5inch 기준)\n",
    "\n",
    "    # **Save Figure (MATLAB saveas와 유사)**\n",
    "    plt.savefig(f\"./figs/{f_name}.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, kernel, mode='same')  # 'valid'는 경계 제외\n",
    "\n",
    "\n",
    "def get_data_from_info(info):\n",
    "    # Action info\n",
    "    action = np.array([_info['action'] for _info in info])\n",
    "\n",
    "    # Status info\n",
    "    stat_init_rpy = np.array([_info['init_rpy'] for _info in info])\n",
    "    stat_init_com = np.array([_info['init_com'] for _info in info])\n",
    "    stat_xy_vel = np.array([[_info['x_velocity'], _info['y_velocity']] for _info in info])\n",
    "    stat_yaw_vel = np.array([_info['yaw_velocity'] for _info in info])\n",
    "    stat_quat = np.array([_info['head_quat'] for _info in info])\n",
    "    stat_ang_vel = np.array([_info['head_ang_vel'] for _info in info])\n",
    "    stat_lin_acc = np.array([_info['head_lin_acc'] for _info in info])\n",
    "    stat_motion_vector = np.array([_info['motion_vector'] for _info in info])\n",
    "    stat_com_pos = np.array([_info['com_pos'] for _info in info])\n",
    "    stat_com_ypr = np.array([_info['com_ypr'] for _info in info])\n",
    "    stat_step_ypr = np.array([_info['step_ypr'] for _info in info])\n",
    "    stat_reward_func_orientation = np.array([_info['reward_func_orientation'] for _info in info])\n",
    "    \n",
    "\n",
    "    # Rew info\n",
    "    rew_linear_movement = np.array([_info['reward_linear_movement'] for _info in info])\n",
    "    reward_angular_movement = np.array([_info['reward_angular_movement'] for _info in info])\n",
    "    reward_efficiency = np.array([_info['reward_efficiency'] for _info in info])\n",
    "    reward_healthy = np.array([_info['reward_healthy'] for _info in info])\n",
    "    cost_ctrl = np.array([_info['cost_ctrl'] for _info in info])\n",
    "    cost_unhealthy = np.array([_info['cost_unhealthy'] for _info in info])\n",
    "    cost_orientation = np.array([_info['cost_orientation'] for _info in info])\n",
    "    cost_yaw_vel = np.array([_info['cost_yaw_vel'] for _info in info])\n",
    "    direction_similarity = np.array([_info['direction_similarity'] for _info in info])\n",
    "    rotation_alignment = np.array([_info['rotation_alignment'] for _info in info])\n",
    "    vel_orientation = np.array([_info['velocity_theta'] for _info in info])\n",
    "\n",
    "    # Input info\n",
    "    input_joy = np.array([_info['joy_input'] for _info in info])\n",
    "    gait_param = np.array([_info['gait_params'] for _info in info])\n",
    "\n",
    "    data_dict = {\n",
    "        'action': action,\n",
    "        'stat_init_rpy': stat_init_rpy,\n",
    "        'stat_init_com': stat_init_com,\n",
    "        'stat_xy_vel': stat_xy_vel,\n",
    "        'stat_yaw_vel': stat_yaw_vel,\n",
    "        'stat_quat': stat_quat,\n",
    "        'stat_ang_vel': stat_ang_vel,\n",
    "        'stat_lin_acc': stat_lin_acc,\n",
    "        'stat_motion_vector': stat_motion_vector,\n",
    "        'stat_com_pos': stat_com_pos,\n",
    "        'stat_com_ypr': stat_com_ypr,\n",
    "        'stat_com_r_ypr':stat_reward_func_orientation,\n",
    "        'stat_step_ypr': stat_step_ypr,\n",
    "\n",
    "        'rew_linear_movement': rew_linear_movement,\n",
    "        'reward_angular_movement': reward_angular_movement,\n",
    "        'reward_efficiency': reward_efficiency,\n",
    "        'reward_healthy': reward_healthy,\n",
    "        'cost_ctrl': cost_ctrl,\n",
    "        'cost_unhealthy': cost_unhealthy,\n",
    "        'cost_orientation': cost_orientation,\n",
    "        'cost_yaw_vel': cost_yaw_vel,\n",
    "        'direction_similarity': direction_similarity,\n",
    "        'rotation_alignment': rotation_alignment,\n",
    "        'vel_orientation': vel_orientation,\n",
    "\n",
    "        'input_joy': input_joy,\n",
    "        'gait_param': gait_param,\n",
    "    }\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517ec57-16da-4789-a76f-2c77998e7a5e",
   "metadata": {},
   "source": [
    "## Ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31cbbe-264f-4298-a2f1-471cf823d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(dashboard_host=\"0.0.0.0\", dashboard_port=8265)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39407944-23a9-42b0-854b-c19f1c43bcdc",
   "metadata": {},
   "source": [
    "## Gym 환경 등록하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809e905-5daf-45c4-919f-98e53d97572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"forward_reward_weight\": 100.0,\n",
    "    \"rotation_reward_weight\": 100.0,\n",
    "    \"unhealthy_max_steps\": 150.0,\n",
    "    \"healthy_reward\": 3.0,\n",
    "    \"healthy_roll_range\": (-40,40),\n",
    "    \"terminating_roll_range\": (-85,85),\n",
    "    \"rotation_norm_cost_weight\": 4.5,\n",
    "    \"termination_reward\": 0,\n",
    "    \"gait_params\": (30, 30, 40, 40, 45),\n",
    "    \"use_friction_chg\": True,\n",
    "    \"joy_input_random\": True,\n",
    "    \"use_imu_window\": True,\n",
    "    \"use_vels_window\": True,\n",
    "    \"ctrl_cost_weight\": 0.05,\n",
    "}\n",
    "\n",
    "render_env_config = env_config.copy()\n",
    "render_env_config['render_mode'] = 'rgb_array'\n",
    "render_env_config['render_camera_name'] = 'ceiling'\n",
    "\n",
    "# env = gym.make(\"horcrux_terrain_v2/plane-v2\", **render_env_config)\n",
    "\n",
    "# JoyWorld\n",
    "register_env(\"joy-v1\", lambda config: PlaneJoyDirWorld( forward_reward_weight=env_config[\"forward_reward_weight\"], \n",
    "                                                     rotation_reward_weight=env_config[\"rotation_reward_weight\"], \n",
    "                                                     unhealthy_max_steps=env_config[\"unhealthy_max_steps\"],\n",
    "                                                     healthy_reward=env_config[\"healthy_reward\"], \n",
    "                                                     healthy_roll_range=env_config[\"healthy_roll_range\"],\n",
    "                                                     terminating_roll_range=env_config[\"terminating_roll_range\"],\n",
    "                                                     rotation_norm_cost_weight=env_config[\"rotation_norm_cost_weight\"],\n",
    "                                                     termination_reward=env_config[\"termination_reward\"],\n",
    "                                                     gait_params=env_config[\"gait_params\"],\n",
    "                                                     use_friction_chg=env_config[\"use_friction_chg\"],\n",
    "                                                     joy_input_random=env_config[\"joy_input_random\"],\n",
    "                                                     use_imu_window=env_config[\"use_imu_window\"],\n",
    "                                                     ctrl_cost_weight=env_config[\"ctrl_cost_weight\"],\n",
    "                                                   )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf2134-3167-46f1-b28b-7d305065f559",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 설정하기 PPO\n",
    "+ 신형 API 구조 사용해보기 (get_policy() 메서드 오류 생김... 추론 못함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436edebc-bcaa-45fa-9941-58ad2868655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.rllib.core.rl_module import RLModuleSpec\n",
    "# from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (\n",
    "#     PPOTorchRLModule\n",
    "# )\n",
    "\n",
    "# from ray.rllib.examples.rl_modules.classes.lstm_containing_rlm import (\n",
    "#     LSTMContainingRLModule,\n",
    "# )\n",
    "# from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "\n",
    "# config = PPOConfig()\n",
    "\n",
    "# # 구형 API 구조 사용\n",
    "# config.api_stack(\n",
    "#     enable_rl_module_and_learner=True,\n",
    "#     enable_env_runner_and_connector_v2=True,\n",
    "# )\n",
    "\n",
    "# config.environment(\"joy-v1\")\n",
    "# config.framework(\"torch\")\n",
    "\n",
    "# # 병렬 CPU 사용 설정\n",
    "# total_workers = 16\n",
    "# config.learners(num_learners = 1, num_gpus_per_learner=1)\n",
    "# config.env_runners(num_env_runners = total_workers, num_cpus_per_env_runner = 1, rollout_fragment_length = 'auto')\n",
    "# config.rl_module(\n",
    "#     rl_module_spec=RLModuleSpec(\n",
    "#         module_class=LSTMContainingRLModule,\n",
    "#         learner_only=False,\n",
    "#         inference_only=False,\n",
    "#     ),\n",
    "#     model_config = DefaultModelConfig(\n",
    "#         fcnet_hiddens=[512, 512, 512, 512, 512, 512],\n",
    "#         fcnet_activation=\"swish\",\n",
    "#         use_lstm=True,\n",
    "#         max_seq_len=100,\n",
    "#         lstm_use_prev_action=True,\n",
    "#         lstm_cell_size=256,\n",
    "#     ),\n",
    "# )\n",
    "# config.training(\n",
    "#     # Default config sets\n",
    "#     gamma=0.95,\n",
    "#     lr=0.0001,\n",
    "#     train_batch_size_per_learner = 100000,\n",
    "#     minibatch_size = 5000,\n",
    "#     num_epochs = 10,\n",
    "#     shuffle_batch_per_epoch = False,\n",
    "\n",
    "#     # PPO config sets\n",
    "#     entropy_coeff = 0.01,\n",
    "#     vf_loss_coeff = 0.5, #이 값 튜닝 진행해야함. (기본값 : 1.0)\n",
    "#     vf_clip_param = 5,\n",
    "# )\n",
    "\n",
    "# algo = config.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d102417",
   "metadata": {},
   "source": [
    "+ 구형 API 사용해서 구현\n",
    "RNN 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "config = PPOConfig()\n",
    "\n",
    "# 구형 API 구조 사용\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=False,\n",
    "    enable_env_runner_and_connector_v2=False,\n",
    ")\n",
    "\n",
    "config.environment(\"joy-v1\")\n",
    "config.framework(\"torch\")\n",
    "config.resources(\n",
    "    num_cpus_for_main_process=4,\n",
    "    num_gpus=1,\n",
    "    num_gpus_per_learner_worker=1,\n",
    ")\n",
    "\n",
    "# 병렬 CPU 사용 설정\n",
    "total_workers = 20\n",
    "config.learners(num_learners = 1, num_gpus_per_learner=1)\n",
    "config.env_runners(num_env_runners = total_workers, num_cpus_per_env_runner = 1, rollout_fragment_length = 'auto')\n",
    "\n",
    "config.training(\n",
    "    # Default config sets\n",
    "    gamma=0.95,\n",
    "    lr=0.0001,\n",
    "    train_batch_size = 100000,\n",
    "    minibatch_size = 5000,\n",
    "    num_epochs = 40,\n",
    "    shuffle_batch_per_epoch = True,\n",
    "    model = {\n",
    "        # \"fcnet_hiddens\": [256, 256, 256, 256, 64],\n",
    "        \"fcnet_hiddens\": [512, 512, 512, 512, 512, 512, 512, 512, 32],\n",
    "        \"fcnet_activation\": \"tanh\",\n",
    "        # \"post_fcnet_hiddens\": [],\n",
    "        # \"post_fcnet_activation\": \"tanh\",\n",
    "        'vf_share_layers': False, #원래는 False로 학습했었음.\n",
    "        \"use_lstm\": False,\n",
    "        # \"max_seq_len\": 40,\n",
    "        # \"lstm_use_prev_action\": True,\n",
    "        # \"lstm_cell_size\": 64,\n",
    "\n",
    "        # \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "        # \"custom_model_config\": {},\n",
    "    },\n",
    "\n",
    "    # PPO config sets\n",
    "    clip_param=0.5,                # 기본값 0.3\n",
    "    entropy_coeff = 0.03,          # 기본값 0.01\n",
    "    kl_coeff=0.3,                  # 기본값 0.2\n",
    "    lambda_=1.0,                   # 기본값 1.0\n",
    "    vf_loss_coeff = 1.0,           # 이 값 튜닝 진행해야함. (기본값 : 1.0)\n",
    "    vf_clip_param = 5.0,            # 기본값 5\n",
    "    grad_clip=0.5,                # 기본값 0.5\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "try:\n",
    "    if prior_weight:\n",
    "        algo.set_weights(prior_weight)\n",
    "        print(\"Prior weight is set to loaded weight.\")\n",
    "\n",
    "except:\n",
    "    print(\"Prior weight does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.get_policy().model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe669f",
   "metadata": {},
   "source": [
    "+ RNN 미사용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beae10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig()\n",
    "\n",
    "# 구형 API 구조 사용\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=False,\n",
    "    enable_env_runner_and_connector_v2=False,\n",
    ")\n",
    "\n",
    "config.environment(\"joy-v1\")\n",
    "config.framework(\"torch\")\n",
    "config.resources(\n",
    "    num_cpus_for_main_process=8,\n",
    "    num_gpus=1,\n",
    "    # num_learner_workers=1,\n",
    "    # num_gpus_per_learner_worker=1,\n",
    ")\n",
    "\n",
    "config.learners(\n",
    "    num_learners=0,\n",
    "    num_gpus_per_learner=1,\n",
    ")\n",
    "\n",
    "# 병렬 CPU 사용 설정\n",
    "total_workers = 16\n",
    "config.env_runners(num_env_runners = total_workers, num_cpus_per_env_runner = 1, rollout_fragment_length = 'auto')\n",
    "\n",
    "config.training(\n",
    "    # Default config sets\n",
    "    gamma=0.95,\n",
    "    lr=0.0005,\n",
    "    train_batch_size = 100000,\n",
    "    minibatch_size = 10000,\n",
    "    num_epochs = 10,\n",
    "    shuffle_batch_per_epoch = False,\n",
    "    model = {\n",
    "        \"fcnet_hiddens\": [512, 512, 512, 512, 512, 512],\n",
    "        \"fcnet_activation\": \"swish\",\n",
    "        # \"post_fcnet_hiddens\": [],\n",
    "        # \"post_fcnet_activation\": \"tanh\",\n",
    "\n",
    "        # \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "        # \"custom_model_config\": {},\n",
    "    },\n",
    "\n",
    "    # PPO config sets\n",
    "    entropy_coeff = 0.01,\n",
    "    vf_loss_coeff = 0.5, #이 값 튜닝 진행해야함. (기본값 : 1.0)\n",
    "    vf_clip_param = 5,\n",
    ")\n",
    "\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba781dc0",
   "metadata": {},
   "source": [
    "+ 학습된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/bong/Project/snake_RL/GD_tor/learning/ray239-ppo'\n",
    "\n",
    "## LP Final\n",
    "# GD_tor/learning/ray239-ppo/learned_policy/20250418-LP-Final/PPO_NEW_512_7_416_vf_noshare_38\n",
    "## LP Best\n",
    "# GD_tor/learning/ray239-ppo/learned_policy/20250418-LP-Final/PPO_NEW_512_7_416_vf_noshare_2_best\n",
    "\n",
    "algo = Algorithm.from_checkpoint(path=base_path + '/learned_policy/20250418-LP-Final/PPO_NEW_512_7_416_vf_noshare_2_best/')\n",
    "\n",
    "# config = algo.get_config()\n",
    "# prior_weight = algo.get_weights()\n",
    "# algo.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefac126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "\n",
    "# pprint(DefaultModelConfig())\n",
    "# pprint(algo.get_config().to_dict())\n",
    "# algo.get_default_policy_class(config).\n",
    "# algo.get_module()\n",
    "\n",
    "# algo.get_config().to_dict()\n",
    "# algo.get_module().get_initial_state()\n",
    "# algo.get_metadata()\n",
    "algo.get_policy().model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae756f-bb08-4813-bf07-c019392dac99",
   "metadata": {},
   "source": [
    "## 학습시작\n",
    "+ 구형 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ae0dfe-e461-442b-b090-cc437723543f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m save_name = \u001b[33m\"\u001b[39m\u001b[33mPPO_8DIR_512_8_v3update_424\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     result = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# result.pop(\"config\")\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# pprint(result)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/tune/trainable/trainable.py:328\u001b[39m, in \u001b[36mTrainable.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    326\u001b[39m start = time.time()\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    330\u001b[39m     skipped = skip_exceptions(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:933\u001b[39m, in \u001b[36mAlgorithm.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    923\u001b[39m     (\n\u001b[32m    924\u001b[39m         train_results,\n\u001b[32m    925\u001b[39m         eval_results,\n\u001b[32m    926\u001b[39m         train_iter_ctx,\n\u001b[32m    927\u001b[39m     ) = \u001b[38;5;28mself\u001b[39m._run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[32m    929\u001b[39m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m     train_results, train_iter_ctx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.evaluation_parallel_to_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:3498\u001b[39m, in \u001b[36mAlgorithm._run_one_training_iteration\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3494\u001b[39m \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._timers[TRAINING_STEP_TIMER]:\n\u001b[32m   3496\u001b[39m     \u001b[38;5;66;03m# TODO (sven): Should we reduce the different\u001b[39;00m\n\u001b[32m   3497\u001b[39m     \u001b[38;5;66;03m#  `training_step_results` over time with MetricsLogger.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3498\u001b[39m     training_step_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_results:\n\u001b[32m   3501\u001b[39m     results = training_step_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo.py:417\u001b[39m, in \u001b[36mPPO.training_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._training_step_new_api_stack()\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Old API stack (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_training_step_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/ppo.py:528\u001b[39m, in \u001b[36mPPO._training_step_old_api_stack\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    522\u001b[39m     train_batch = synchronous_parallel_sample(\n\u001b[32m    523\u001b[39m         worker_set=\u001b[38;5;28mself\u001b[39m.env_runner_group,\n\u001b[32m    524\u001b[39m         max_agent_steps=\u001b[38;5;28mself\u001b[39m.config.total_train_batch_size,\n\u001b[32m    525\u001b[39m         sample_timeout_s=\u001b[38;5;28mself\u001b[39m.config.sample_timeout_s,\n\u001b[32m    526\u001b[39m     )\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     train_batch = \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Return early if all our workers failed.\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py:106\u001b[39m, in \u001b[36msynchronous_parallel_sample\u001b[39m\u001b[34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, random_actions, _uses_new_env_runners, _return_metrics)\u001b[39m\n\u001b[32m    103\u001b[39m         stats_dicts = [worker_set.local_env_runner.get_metrics()]\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     sampled_data = \u001b[43mworker_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set.num_healthy_remote_workers() <= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py:896\u001b[39m, in \u001b[36mEnvRunnerGroup.foreach_worker\u001b[39m\u001b[34m(self, func, local_env_runner, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy, local_worker)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._worker_manager.actor_ids():\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_worker_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m FaultTolerantActorManager.handle_remote_call_result_errors(\n\u001b[32m    906\u001b[39m     remote_results, ignore_ray_errors=\u001b[38;5;28mself\u001b[39m._ignore_ray_errors_on_env_runners\n\u001b[32m    907\u001b[39m )\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py:452\u001b[39m, in \u001b[36mFaultTolerantActorManager.foreach_actor\u001b[39m\u001b[34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    446\u001b[39m remote_calls = \u001b[38;5;28mself\u001b[39m._call_actors(\n\u001b[32m    447\u001b[39m     func=func,\n\u001b[32m    448\u001b[39m     remote_actor_ids=remote_actor_ids,\n\u001b[32m    449\u001b[39m )\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m _, remote_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py:770\u001b[39m, in \u001b[36mFaultTolerantActorManager._fetch_result\u001b[39m\u001b[34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[39m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m readies, _ = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[32m    779\u001b[39m remote_results = RemoteCallResults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:21\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     20\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gd239/lib/python3.12/site-packages/ray/_private/worker.py:2984\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[39m\n\u001b[32m   2982\u001b[39m timeout = timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m10\u001b[39m**\u001b[32m6\u001b[39m\n\u001b[32m   2983\u001b[39m timeout_milliseconds = \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2984\u001b[39m ready_ids, remaining_ids = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/_raylet.pyx:3816\u001b[39m, in \u001b[36mray._raylet.CoreWorker.wait\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/includes/common.pxi:79\u001b[39m, in \u001b[36mray._raylet.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import datetime\n",
    "from scipy.io import savemat\n",
    "\n",
    "n_iter = 1000\n",
    "save_iter = 0\n",
    "save_name = \"PPO_8DIR_512_8_v3update_424\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(f\"{i:03d}\", end=\", \")\n",
    "    # result.pop(\"config\")\n",
    "    # pprint(result)\n",
    "\n",
    "    if i%50 == 0:\n",
    "        checkpoint_dir = algo.save(save_name + \"_\" + str(save_iter))\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        save_iter += 1\n",
    "\n",
    "\n",
    "        # Record Validation Env\n",
    "        env = gym.make(\"horcrux_terrain_v2/plane-v3\", **render_env_config)\n",
    "        obs = env.reset()[0]\n",
    "        env_done = False\n",
    "        init_prev_a = prev_a = np.array([0]*14)\n",
    "        lstm_cell_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "\n",
    "        if algo.config.enable_rl_module_and_learner:\n",
    "            init_state = state = algo.get_policy().model.get_initial_state()\n",
    "        else:\n",
    "            init_state = state = [np.zeros([lstm_cell_size], np.float32) for _ in range(2)]\n",
    "\n",
    "        rew_return = 0\n",
    "        frames = []\n",
    "        info = []\n",
    "\n",
    "        for i in range(3000):\n",
    "            act, _state_out, _ = algo.compute_single_action(observation=obs, state=state, prev_action=prev_a, explore=False)\n",
    "            obs, _step_rew, _, env_done, env_info = env.step(act)\n",
    "            pixels = env.render()\n",
    "            frames.append(pixels)\n",
    "            info.append(env_info)\n",
    "            rew_return += _step_rew\n",
    "            state = _state_out\n",
    "            prev_a = act\n",
    "\n",
    "        _video_base_name = 'rl-video'\n",
    "\n",
    "        _f_name, _full_path = get_unique_filename(f\"./video/{_video_base_name}\")\n",
    "        rew_dict = get_data_from_info(info)\n",
    "        rew_dict['rew_return'] = rew_return\n",
    "        rew_dict['motionMatrix'] = info[-1]['motionMatrix']\n",
    "\n",
    "        # Save Video\n",
    "        save_video(frames, \"./video/\", name_prefix=_f_name, fps=env.metadata['render_fps'])\n",
    "\n",
    "        # Save Video Info\n",
    "        _f_video_info = open(f\"./video/joy_input.txt\", 'a')\n",
    "        _f_video_info.write(f'File creation time: {datetime.datetime.now()}\\n')\n",
    "        _f_video_info.write(f'Video file name: {_f_name}, Joy input: {info[0][\"joy_input\"]}, Friction: {info[0][\"friction_coeff\"]}\\n')\n",
    "        _f_video_info.close()\n",
    "\n",
    "        # Save Reward Info mat file\n",
    "        savemat(f\"./data/{save_name}_{_f_name}.mat\", rew_dict)\n",
    "\n",
    "        env.reset()\n",
    "        env.close()\n",
    "\n",
    "\n",
    "algo.save(save_name + str(\"_final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7a38d",
   "metadata": {},
   "source": [
    "정책 녹화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import datetime\n",
    "from scipy.io import savemat\n",
    "\n",
    "config = algo.get_config()\n",
    "\n",
    "save_name = \"8DIR-1-0\"\n",
    "num_videos = 30\n",
    "for iteration in range(num_videos):\n",
    "    # Record Validation Env\n",
    "    env = gym.make(\"horcrux_terrain_v2/plane-v3\", **render_env_config)\n",
    "    obs = env.reset()[0]\n",
    "    env_done = False\n",
    "    init_prev_a = prev_a = np.array([0]*14)\n",
    "    lstm_cell_size = config[\"model\"][\"lstm_cell_size\"]\n",
    "\n",
    "    if algo.config.enable_rl_module_and_learner:\n",
    "        init_state = state = algo.get_policy().model.get_initial_state()\n",
    "    else:\n",
    "        init_state = state = [np.zeros([lstm_cell_size], np.float32) for _ in range(2)]\n",
    "\n",
    "    rew_return = 0\n",
    "    frames = []\n",
    "    info = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        act, _state_out, _ = algo.compute_single_action(observation=obs, state=state, prev_action=prev_a)\n",
    "        obs, _step_rew, _, env_done, env_info = env.step(act)\n",
    "        pixels = env.render()\n",
    "        frames.append(pixels)\n",
    "        info.append(env_info)\n",
    "        rew_return += _step_rew\n",
    "        state = _state_out\n",
    "        prev_a = act\n",
    "\n",
    "    _video_base_name = 'rl-video'\n",
    "\n",
    "    _f_name, _full_path = get_unique_filename(f\"./video/{_video_base_name}\")\n",
    "    rew_dict = get_data_from_info(info)\n",
    "    rew_dict['rew_return'] = rew_return\n",
    "\n",
    "    # Save Video\n",
    "    save_video(frames, \"./video/\", name_prefix=_f_name, fps=env.metadata['render_fps'])\n",
    "\n",
    "    # Save Video Info\n",
    "    _f_video_info = open(f\"./video/joy_input.txt\", 'a')\n",
    "    _f_video_info.write(f'File creation time: {datetime.datetime.now()}\\n')\n",
    "    _f_video_info.write(f'Video file name: {_f_name}, Joy input: {info[0][\"joy_input\"]}, Friction: {info[0][\"friction_coeff\"]}\\n')\n",
    "    _f_video_info.close()\n",
    "\n",
    "    # Save Reward Info mat file\n",
    "    savemat(f\"./data/{save_name}_{_f_name}.mat\", rew_dict)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46573140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.get_module().input_specs_train\n",
    "# algo.get_module().input_specs_inference()\n",
    "\n",
    "\n",
    "algo.get_policy().model.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf445a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd239",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
