{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 진짜 완전 새롭게 다시 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from horcrux_terrain_v2.envs import PlaneJoyWorld\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.sac import SACConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import psutil\n",
    "\n",
    "# conn_ip = \"\"\n",
    "# interfaces = psutil.net_if_addrs()\n",
    "# for interface_name, addresses in interfaces.items():\n",
    "#     if \"openvpn\" in interface_name.lower() and \"tap\" in interface_name.lower():\n",
    "#         snicaddrs = interfaces[str(interface_name)]\n",
    "#         for addrfamily in snicaddrs:\n",
    "#             if addrfamily.family == socket.AF_INET:\n",
    "#                 conn_ip = addrfamily.address\n",
    "\n",
    "# 해당 init을 통해서 VPN을 통한 외부 접속 가능함.\n",
    "ray.init(dashboard_host=\"0.0.0.0\", dashboard_port=8265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"forward_reward_weight\": 6.5,\n",
    "    \"side_cost_weight\": 2.0,\n",
    "    \"unhealthy_max_steps\": 100,\n",
    "    \"healthy_reward\": 0.5,\n",
    "    \"healthy_roll_range\": (-35,35),\n",
    "    \"terminating_roll_range\": (-85,85),\n",
    "    \"rotation_norm_cost_weight\": 0.01,\n",
    "    \"rotation_orientation_cost_weight\": 1.2,\n",
    "    \"termination_reward\": 0,\n",
    "    \"gait_params\": (30, 30, 60, 60, 0),\n",
    "    \"use_friction_chg\": True,\n",
    "    \"joy_input_random\": True,\n",
    "}\n",
    "\n",
    "# JoyWorld\n",
    "register_env(\"joy-v1\", lambda config: PlaneJoyWorld( forward_reward_weight=env_config[\"forward_reward_weight\"], \n",
    "                                                     side_cost_weight=env_config[\"side_cost_weight\"], \n",
    "                                                     unhealthy_max_steps=env_config[\"unhealthy_max_steps\"],\n",
    "                                                     healthy_reward=env_config[\"healthy_reward\"], \n",
    "                                                     healthy_roll_range=env_config[\"healthy_roll_range\"],\n",
    "                                                     terminating_roll_range=env_config[\"terminating_roll_range\"],\n",
    "                                                     rotation_norm_cost_weight=env_config[\"rotation_norm_cost_weight\"],\n",
    "                                                     rotation_orientation_cost_weight=env_config[\"rotation_orientation_cost_weight\"],\n",
    "                                                     termination_reward=env_config[\"termination_reward\"],\n",
    "                                                     gait_params=env_config[\"gait_params\"],\n",
    "                                                     use_friction_chg=env_config[\"use_friction_chg\"],\n",
    "                                                     joy_input_random=env_config[\"joy_input_random\"],\n",
    "                                                   )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\"joy-v1\")\n",
    "    .env_runners(num_env_runners=16, num_envs_per_env_runner=4)\n",
    "    .framework('torch')\n",
    "    .learners(num_learners=1, num_gpus_per_learner=1)\n",
    "    .training(\n",
    "        gamma=0.9,\n",
    "        actor_lr=0.001,\n",
    "        critic_lr=0.002,\n",
    "        train_batch_size_per_learner= 32768,\n",
    "\n",
    "        replay_buffer_config={\n",
    "            \"_enable_replay_buffer_api\": True,\n",
    "            # \"type\": \"EpisodeReplayBuffer\",\n",
    "            \"capacity\": int(4e7),\n",
    "            \"replay_batch_size\": 1024,\n",
    "            # \"replay_sequence_length\": 1,\n",
    "        },\n",
    "\n",
    "        q_model_config = {\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "        policy_model_config = {\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"custom_model\": None,  # Use this to define a custom policy model.\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build the SAC algo object from the config and run 1 training iteration.\n",
    "algo = config.build_algo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "n_iter = 4000\n",
    "save_iter = 0\n",
    "save_name = \"~/learned_policy/SAC_layer_512_5_32_linear_friction_joy_314\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(f\"{i:03d}\", end=\", \")\n",
    "    # result.pop(\"config\")\n",
    "    # pprint(result)\n",
    "\n",
    "    if i%40 == 0:\n",
    "        checkpoint_dir = algo.save(checkpoint_dir=save_name+f\"_{save_iter}\")\n",
    "        pprint(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        save_iter += 1\n",
    "\n",
    "checkpoint_dir = algo.save(checkpoint_dir=save_name+\"_final\")\n",
    "pprint(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
