{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 진짜 완전 새롭게 다시 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from horcrux_terrain_v2.envs import PlaneJoyWorld\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.sac import SACConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 10:44:14,627\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GPU': 1.0, 'accelerator_type:G': 1.0, 'memory': 28693816935.0, 'CPU': 24.0, 'object_store_memory': 14346908467.0, 'node:172.30.151.148': 1.0, 'node:__internal_head__': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import psutil\n",
    "\n",
    "# conn_ip = \"\"\n",
    "# interfaces = psutil.net_if_addrs()\n",
    "# for interface_name, addresses in interfaces.items():\n",
    "#     if \"openvpn\" in interface_name.lower() and \"tap\" in interface_name.lower():\n",
    "#         snicaddrs = interfaces[str(interface_name)]\n",
    "#         for addrfamily in snicaddrs:\n",
    "#             if addrfamily.family == socket.AF_INET:\n",
    "#                 conn_ip = addrfamily.address\n",
    "\n",
    "# 해당 init을 통해서 VPN을 통한 외부 접속 가능함.\n",
    "ray.init(dashboard_host=\"0.0.0.0\", dashboard_port=8265)\n",
    "print(ray.available_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"forward_reward_weight\": 6.5,\n",
    "    \"side_cost_weight\": 2.0,\n",
    "    \"unhealthy_max_steps\": 100,\n",
    "    \"healthy_reward\": 0.5,\n",
    "    \"healthy_roll_range\": (-35,35),\n",
    "    \"terminating_roll_range\": (-85,85),\n",
    "    \"rotation_norm_cost_weight\": 0.01,\n",
    "    \"rotation_orientation_cost_weight\": 1.2,\n",
    "    \"termination_reward\": 0,\n",
    "    \"gait_params\": (30, 30, 60, 60, 0),\n",
    "    \"use_friction_chg\": True,\n",
    "    \"joy_input_random\": True,\n",
    "}\n",
    "\n",
    "# JoyWorld\n",
    "register_env(\"joy-v1\", lambda config: PlaneJoyWorld( forward_reward_weight=env_config[\"forward_reward_weight\"], \n",
    "                                                     side_cost_weight=env_config[\"side_cost_weight\"], \n",
    "                                                     unhealthy_max_steps=env_config[\"unhealthy_max_steps\"],\n",
    "                                                     healthy_reward=env_config[\"healthy_reward\"], \n",
    "                                                     healthy_roll_range=env_config[\"healthy_roll_range\"],\n",
    "                                                     terminating_roll_range=env_config[\"terminating_roll_range\"],\n",
    "                                                     rotation_norm_cost_weight=env_config[\"rotation_norm_cost_weight\"],\n",
    "                                                     rotation_orientation_cost_weight=env_config[\"rotation_orientation_cost_weight\"],\n",
    "                                                     termination_reward=env_config[\"termination_reward\"],\n",
    "                                                     gait_params=env_config[\"gait_params\"],\n",
    "                                                     use_friction_chg=env_config[\"use_friction_chg\"],\n",
    "                                                     joy_input_random=env_config[\"joy_input_random\"],\n",
    "                                                   )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are using the new API stack EnvRunners (SingleAgentEnvRunner or MultiAgentEnvRunner), but have forgotten to switch on the new API stack! Try setting `config.api_stack(enable_rl_module_and_learner=True)`.\nTo suppress all validation errors, set `config.experimental(_validate_config=False)` at your own risk.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m      1\u001b[39m config = (\n\u001b[32m      2\u001b[39m     SACConfig()\n\u001b[32m      3\u001b[39m     .environment(\u001b[33m\"\u001b[39m\u001b[33mjoy-v1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     )\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Build the SAC algo object from the config and run 1 training iteration.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# config.to_dict()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:957\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    955\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/sac/sac.py:570\u001b[39m, in \u001b[36mSAC.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m._allow_unknown_subkeys += [\u001b[33m\"\u001b[39m\u001b[33mpolicy_model_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mq_model_config\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:519\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m     config.environment(env)\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Validate and freeze our AlgorithmConfig object (no more changes possible).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m config.freeze()\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# Convert `env` provided in config into a concrete env creator callable, which\u001b[39;00m\n\u001b[32m    523\u001b[39m \u001b[38;5;66;03m# takes an EnvContext (config dict) as arg and returning an RLlib supported Env\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# type (e.g. a gym.Env).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/sac/sac.py:386\u001b[39m, in \u001b[36mSACConfig.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m(AlgorithmConfig)\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    385\u001b[39m     \u001b[38;5;66;03m# Call super's validation method.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m     \u001b[38;5;66;03m# Check rollout_fragment_length to be compatible with n_step.\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_step, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:919\u001b[39m, in \u001b[36mAlgorithmConfig.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_evaluation_settings()\n\u001b[32m    918\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_offline_settings()\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_new_api_stack_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_to_be_deprecated_settings()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:4715\u001b[39m, in \u001b[36mAlgorithmConfig._validate_new_api_stack_settings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4712\u001b[39m \u001b[38;5;66;03m# User is using the new EnvRunners, but forgot to switch on\u001b[39;00m\n\u001b[32m   4713\u001b[39m \u001b[38;5;66;03m# `enable_rl_module_and_learner`.\u001b[39;00m\n\u001b[32m   4714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_env_runner_and_connector_v2:\n\u001b[32m-> \u001b[39m\u001b[32m4715\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4716\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are using the new API stack EnvRunners (SingleAgentEnvRunner \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   4717\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mor MultiAgentEnvRunner), but have forgotten to switch on the new \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   4718\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAPI stack! Try setting \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   4719\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m`config.api_stack(enable_rl_module_and_learner=True)`.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   4720\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4721\u001b[39m \u001b[38;5;66;03m# Early out. The rest of this method is only for\u001b[39;00m\n\u001b[32m   4722\u001b[39m \u001b[38;5;66;03m# `enable_rl_module_and_learner=True`.\u001b[39;00m\n\u001b[32m   4723\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/wsl/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:4464\u001b[39m, in \u001b[36mAlgorithmConfig._value_error\u001b[39m\u001b[34m(self, errmsg)\u001b[39m\n\u001b[32m   4459\u001b[39m msg = errmsg + (\n\u001b[32m   4460\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTo suppress all validation errors, set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4461\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m`config.experimental(_validate_config=False)` at your own risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4462\u001b[39m )\n\u001b[32m   4463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_config:\n\u001b[32m-> \u001b[39m\u001b[32m4464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   4465\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4466\u001b[39m     logger.warning(errmsg)\n",
      "\u001b[31mValueError\u001b[39m: You are using the new API stack EnvRunners (SingleAgentEnvRunner or MultiAgentEnvRunner), but have forgotten to switch on the new API stack! Try setting `config.api_stack(enable_rl_module_and_learner=True)`.\nTo suppress all validation errors, set `config.experimental(_validate_config=False)` at your own risk."
     ]
    }
   ],
   "source": [
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\"joy-v1\")\n",
    "    .env_runners(num_env_runners=24)\n",
    "    .api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)\n",
    "    .resources(num_gpus=1)\n",
    "    .framework('torch')\n",
    "    .training(\n",
    "        gamma=0.9,\n",
    "        actor_lr=0.001,\n",
    "        critic_lr=0.002,\n",
    "        train_batch_size= 100000,\n",
    "        train_batch_size_per_learner= 100000,\n",
    "        num_steps_sampled_before_learning_starts = 200000,\n",
    "        replay_buffer_config={\n",
    "            \"_enable_replay_buffer_api\": True,\n",
    "            # \"type\": \"MultiAgentReplayBuffer\",\n",
    "            # \"type\": \"EpisodeReplayBuffer\",\n",
    "            \"capacity\": int(1000000),\n",
    "            \"replay_batch_size\": 10000,\n",
    "        },\n",
    "\n",
    "        q_model_config = {\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"custom_model\": None,  # Use this to define custom Q-model(s).\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "        policy_model_config = {\n",
    "            \"fcnet_hiddens\": [512, 512, 512, 512, 512, 32],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"custom_model\": None,  # Use this to define a custom policy model.\n",
    "            \"custom_model_config\": {},\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build the SAC algo object from the config and run 1 training iteration.\n",
    "algo = config.build_algo()\n",
    "# config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "n_iter = 4000\n",
    "save_iter = 0\n",
    "save_name = \"~/learned_policy/SAC_layer_512_5_32_linear_friction_joy_320\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(f\"{i:03d}\", end=\", \")\n",
    "    # result.pop(\"config\")\n",
    "    # pprint(result)\n",
    "\n",
    "    if i%200 == 0:\n",
    "        checkpoint_dir = algo.save_to_path(path=save_name+f\"_{save_iter}\")\n",
    "        pprint(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        save_iter += 1\n",
    "\n",
    "checkpoint_dir = algo.save_to_path(path=save_name+\"_final\")\n",
    "pprint(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
