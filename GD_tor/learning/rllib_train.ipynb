{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ray RLlib 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요 패키지 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\framework.py:130: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from horcrux_terrain_v1.envs import SandWorld\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.tune.registry import register_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym -> Rllib Env 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(\"sand-v1\", lambda config: SandWorld())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 알고리즘 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`mini_batch_size_per_learner` (4096) must be <= `train_batch_size_per_learner` (3200). In PPO, the train batch will be split into 4096 chunks, each of which is iterated over (used for updating the policy) 30 times.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m config\u001b[38;5;241m.\u001b[39menv_runners(num_env_runners\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     10\u001b[0m config\u001b[38;5;241m.\u001b[39mtraining(\n\u001b[0;32m     11\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, \n\u001b[0;32m     12\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     sgd_minibatch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, \n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:880\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[1;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    878\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[1;32m--> 880\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:502\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m     config\u001b[38;5;241m.\u001b[39menvironment(env)\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# Validate and freeze our AlgorithmConfig object (no more changes possible).\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m config\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Convert `env` provided in config into a concrete env creator callable, which\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# takes an EnvContext (config dict) as arg and returning an RLlib supported Env\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# type (e.g. a gym.Env).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:357\u001b[0m, in \u001b[0;36mPPOConfig.validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    355\u001b[0m     tbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch_size_per_learner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch_size\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mbs, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tbs, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m mbs \u001b[38;5;241m>\u001b[39m tbs:\n\u001b[1;32m--> 357\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`mini_batch_size_per_learner` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be <= \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`train_batch_size_per_learner` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). In PPO, the train batch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m will be split into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks, each of which is iterated over \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(used for updating the policy) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sgd_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Episodes may only be truncated (and passed into PPO's\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# `postprocessing_fn`), iff generalized advantage estimation is used\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# (value function estimate at end of truncated episode to estimate\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# remaining value).\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_evaluation\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate_episodes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gae\n\u001b[0;32m    372\u001b[0m ):\n",
      "\u001b[1;31mValueError\u001b[0m: `mini_batch_size_per_learner` (4096) must be <= `train_batch_size_per_learner` (3200). In PPO, the train batch will be split into 4096 chunks, each of which is iterated over (used for updating the policy) 30 times."
     ]
    }
   ],
   "source": [
    "config = PPOConfig()\n",
    "# Activate new API stack.\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=True,\n",
    "    enable_env_runner_and_connector_v2=True,\n",
    ")\n",
    "config.environment(\"sand-v1\")\n",
    "config.resources(num_gpus=1, num_cpus_for_main_process=16)\n",
    "config.env_runners(num_env_runners=16)\n",
    "config.training(\n",
    "    gamma=0.9, \n",
    "    lr=0.01, \n",
    "    kl_coeff=0.3, \n",
    "    train_batch_size_per_learner=8192,              \n",
    "    sgd_minibatch_size=4096, \n",
    ")\n",
    "\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "\n",
    "for i in range(n_iter):\n",
    "    result = algo.train()\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
