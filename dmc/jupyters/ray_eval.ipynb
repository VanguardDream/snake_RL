{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bong\\anaconda3\\envs\\rllib\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Bong\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\tune\\logger\\tensorboardx.py:41: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pathlib\n",
    "# # __location__ = os.getcwd()\n",
    "# # __location__ = pathlib.Path(__location__)\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\bong\\\\project\\\\snake_RL\\\\dmc\\\\domains')\n",
    "# import snake\n",
    "# import snake_v2\n",
    "# import snake_v3\n",
    "\n",
    "\n",
    "# from snake.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "# from snake_v2.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "# from snake_v8_mk1.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "# from snake_v8.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "from snake_v8_CG.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "import gymnasium as gym\n",
    "\n",
    "# import rl 알고리즘\n",
    "from ray.rllib.algorithms.ppo import PPOConfig \n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Snake Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('snake/SnakeEnv-mk1-v8', render_mode=\"human\")\n",
    "env = gym.make('snake/SnakeEnv-cg-v8', render_mode=\"human\")\n",
    "# env = gym.make('snake/SnakeEnv-v8', render_mode=\"human\")\n",
    "\n",
    "# register_env(\"snake-mk1-v8\", lambda config: SnakeEnv())\n",
    "register_env(\"snake-CG-v8\", lambda config: SnakeEnv())\n",
    "# register_env(\"snake-v8\", lambda config: SnakeEnv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.recurrent_net import RecurrentNetwork as TorchRNN\n",
    "from ray.rllib.models.tf.recurrent_net import RecurrentNetwork\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "load_policy = Policy.from_checkpoint('C:\\\\Users\\\\Bong\\\\ray_results\\\\PPO_snake_CG_v8_2023-07-14_13-37-46mkbk4o_n\\\\checkpoint_000500')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m load_policy \u001b[39m=\u001b[39m Policy\u001b[39m.\u001b[39mfrom_checkpoint(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mBong\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mray_results\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mPPO_snake-v8_Base_925-1050\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcheckpoint_001000\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Policy' is not defined"
     ]
    }
   ],
   "source": [
    "load_policy = Policy.from_checkpoint('C:\\\\Users\\\\Bong\\\\ray_results\\\\PPO_snake-v8_Base_925-1050\\\\checkpoint_001000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.058235607811014\n",
      "episode done...\n",
      "59.85619134105026\n",
      "episode done...\n",
      "59.950193790534556\n",
      "episode done...\n"
     ]
    }
   ],
   "source": [
    "## For Recurrent Policy\n",
    "\n",
    "# lstm_dim = 64\n",
    "# num_lstm_cells = 2\n",
    "# initial_state = np.zeros((num_lstm_cells, lstm_dim))\n",
    "\n",
    "_state = load_policy['default_policy'].get_initial_state()\n",
    "_prev_action = np.zeros(14,)\n",
    "_reward = 0\n",
    "_obs, _ = env.reset()\n",
    "_accum_reward = 0\n",
    "\n",
    "for epi in range(3):\n",
    "    for i in range(1870):\n",
    "        _act, _state, _ = load_policy['default_policy'].compute_single_action(obs=_obs.copy(), state=_state.copy(), prev_action=_prev_action.copy(), prev_reward=_reward)\n",
    "        _prev_action = _act.copy()\n",
    "        # print(_act)\n",
    "        _obs, _reward, _done, _, _dict = env.step(_act)\n",
    "        _accum_reward = _accum_reward + _reward\n",
    "\n",
    "        if _done:\n",
    "            break\n",
    "\n",
    "        # print(str(_accum_reward)+\"  \\r\",end='')\n",
    "        # print(f\"{_dict['head_rotation']}\")\n",
    "        # print(_dict['forward_reward'])\n",
    "    print(_accum_reward)\n",
    "    print(\"episode done...\")\n",
    "    _obs, _ = env.reset()\n",
    "    _accum_reward = 0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_policy['default_policy'].model.get_parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_obs, _ = env.reset()\n",
    "\n",
    "c_reward = 0\n",
    "for epi in range(10):\n",
    "    for i in range(1400):\n",
    "        _act = load_policy['default_policy'].compute_single_action(_obs)\n",
    "        _obs, _reward, _, _, _ = env.step(_act[0])\n",
    "        c_reward = c_reward + _reward\n",
    "    _obs, _ = env.reset()\n",
    "    print(c_reward)\n",
    "    c_reward = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_state = load_policy['default_policy'].get_initial_state()\n",
    "_state = np.tile(_state,(10,1))\n",
    "# _state = np.concatenate(_state)\n",
    "\n",
    "env.close()\n",
    "# _obs, _ = env.reset()\n",
    "\n",
    "_act, _state, _ = load_policy['default_policy'].compute_single_action(obs=_obs, state=_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d47bedaa1c91cdd818b14831e10e7cdc4f0653b726a0327bbea2da1503325d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
