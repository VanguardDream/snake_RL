{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, os\n",
    "# import pathlib\n",
    "# # __location__ = os.getcwd()\n",
    "# # __location__ = pathlib.Path(__location__)\n",
    "\n",
    "# # sys.path.append('C:\\\\Users\\\\doore\\\\project\\\\snake_RL\\\\dmc\\\\domains')\n",
    "# import snake\n",
    "# import snake_v2\n",
    "# import snake_v3\n",
    "\n",
    "\n",
    "# from snake.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "# from snake_v2.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "from snake_v5.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "import gymnasium as gym\n",
    "\n",
    "# import rl 알고리즘\n",
    "from ray.rllib.algorithms.ppo import PPOConfig \n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Snake Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('snake/SnakeEnv-v5', render_mode=\"human\")\n",
    "\n",
    "register_env(\"snake-v5\", lambda config: SnakeEnv())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.recurrent_net import RecurrentNetwork as TorchRNN\n",
    "from ray.rllib.models.tf.recurrent_net import RecurrentNetwork\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "policy = Policy.from_checkpoint('C:/Users/doore/ray_results/PPO_snake-v5_2023-03-27_02-26-33khkire5h/checkpoint_000450')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.538207062794032    \n",
      "episode done...\n",
      "-18.359133346025587   \n",
      "episode done...\n",
      "3.894964233573446     \n",
      "episode done...\n"
     ]
    }
   ],
   "source": [
    "## For Recurrent Policy\n",
    "\n",
    "# lstm_dim = 64\n",
    "# num_lstm_cells = 2\n",
    "# initial_state = np.zeros((num_lstm_cells, lstm_dim))\n",
    "\n",
    "_state = policy['default_policy'].get_initial_state()\n",
    "_prev_action = np.zeros(14,)\n",
    "_reward = 0\n",
    "_obs, _ = env.reset()\n",
    "_accum_reward = 0\n",
    "\n",
    "for epi in range(3):\n",
    "    for i in range(611):\n",
    "        _act, _state, _ = policy['default_policy'].compute_single_action(obs=_obs.copy(), state=_state.copy(), prev_action=_prev_action.copy(), prev_reward=_reward)\n",
    "        _prev_action = _act.copy()\n",
    "        _obs, _reward, _, _, _dict = env.step(_act)\n",
    "        _accum_reward = _accum_reward + _reward\n",
    "        print(str(_accum_reward)+\"  \\r\",end='')\n",
    "        # print(_dict['forward_reward'])\n",
    "    _obs, _ = env.reset()\n",
    "    _accum_reward = 0\n",
    "    print(\"\\nepisode done...\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy['default_policy'].model.get_parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_obs, _ = env.reset()\n",
    "\n",
    "c_reward = 0\n",
    "for epi in range(10):\n",
    "    for i in range(400):\n",
    "        _act = policy['default_policy'].compute_single_action(_obs)\n",
    "        _obs, _reward, _, _, _ = env.step(_act[0])\n",
    "        c_reward = c_reward + _reward\n",
    "    _obs, _ = env.reset()\n",
    "    print(c_reward)\n",
    "    c_reward = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 01:51:58,101\tERROR tf_run_builder.py:50 -- Error fetching: [<tf.Tensor 'var_scope_1/cond_1/Merge:0' shape=(?, 14) dtype=float32>, <tf.Tensor 'var_scope_1/Reshape_2:0' shape=(?, 32) dtype=float32>, {'action_prob': <tf.Tensor 'var_scope_1/Exp_1:0' shape=(?,) dtype=float32>, 'action_logp': <tf.Tensor 'var_scope_1/cond_2/Merge:0' shape=(?,) dtype=float32>, 'action_dist_inputs': <tf.Tensor 'var_scope_1/model_2/dense_6/BiasAdd:0' shape=(?, 28) dtype=float32>, 'vf_preds': <tf.Tensor 'var_scope_1/Reshape_3:0' shape=(?,) dtype=float32>}], feed_dict={<tf.Tensor 'var_scope_1/obs:0' shape=(?, 38) dtype=float32>: array([[ 0.00000000e+00, -1.00000000e+00, -1.00000000e+00,\n",
      "        -1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
      "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00, -1.00000000e+00, -2.41623026e+00,\n",
      "        -8.01598502e-01,  1.50000000e+01,  2.46497944e-02,\n",
      "        -2.10706904e-02,  2.55857797e-02,  1.93289020e-02,\n",
      "         1.60709652e-02, -1.45440128e-02,  2.29784999e-02,\n",
      "         2.82393251e-02,  1.49455915e-02, -2.12075035e-02,\n",
      "        -1.10059075e-02, -2.18891029e-02, -1.41522471e-03,\n",
      "        -9.48487668e-03,  2.49057762e-02,  2.88091973e-02,\n",
      "        -1.08870860e-02,  9.99852574e-01,  9.54299422e-03,\n",
      "        -2.13626355e-03, -1.41137638e-02]]), <tf.Tensor 'var_scope_1/state_in_0:0' shape=(?, ?, 32) dtype=float32>: array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), <tf.Tensor 'var_scope_1/seq_lens:0' shape=(?,) dtype=int32>: array([1.]), <tf.Tensor 'var_scope_1/is_exploring:0' shape=() dtype=bool>: True, <tf.Tensor 'var_scope_1/timestep:0' shape=() dtype=int64>: 80000}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\", line 42, in get\n",
      "    self._executed = _run_timeline(\n",
      "  File \"c:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py\", line 112, in _run_timeline\n",
      "    fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "  File \"c:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 968, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"c:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1165, in _run\n",
      "    raise ValueError(\n",
      "ValueError: Cannot feed value of shape (1, 32) for Tensor var_scope_1/state_in_0:0, which has shape (?, ?, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 32) for Tensor var_scope_1/state_in_0:0, which has shape (?, ?, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      6\u001b[0m \u001b[39m# _obs, _ = env.reset()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m _act, _state, _ \u001b[39m=\u001b[39m policy[\u001b[39m'\u001b[39;49m\u001b[39mdefault_policy\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcompute_single_action(obs\u001b[39m=\u001b[39;49m_obs, state\u001b[39m=\u001b[39;49m_state)\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\policy\\policy.py:507\u001b[0m, in \u001b[0;36mPolicy.compute_single_action\u001b[1;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m     episodes \u001b[39m=\u001b[39m [episode]\n\u001b[1;32m--> 507\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_actions_from_input_dict(\n\u001b[0;32m    508\u001b[0m     input_dict\u001b[39m=\u001b[39;49mSampleBatch(input_dict),\n\u001b[0;32m    509\u001b[0m     episodes\u001b[39m=\u001b[39;49mepisodes,\n\u001b[0;32m    510\u001b[0m     explore\u001b[39m=\u001b[39;49mexplore,\n\u001b[0;32m    511\u001b[0m     timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[0;32m    512\u001b[0m )\n\u001b[0;32m    514\u001b[0m \u001b[39m# Some policies don't return a tuple, but always just a single action.\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39m# E.g. ES and ARS.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\policy\\tf_policy.py:326\u001b[0m, in \u001b[0;36mTFPolicy.compute_actions_from_input_dict\u001b[1;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m to_fetch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_compute_actions(\n\u001b[0;32m    322\u001b[0m     builder, input_dict\u001b[39m=\u001b[39minput_dict, explore\u001b[39m=\u001b[39mexplore, timestep\u001b[39m=\u001b[39mtimestep\n\u001b[0;32m    323\u001b[0m )\n\u001b[0;32m    325\u001b[0m \u001b[39m# Execute session run to get action (and other fetches).\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m fetched \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mget(to_fetch)\n\u001b[0;32m    328\u001b[0m \u001b[39m# Update our global timestep by the batch size.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_timestep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    330\u001b[0m     \u001b[39mlen\u001b[39m(obs_batch)\n\u001b[0;32m    331\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs_batch, \u001b[39mlist\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[39melse\u001b[39;00m obs_batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    335\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py:55\u001b[0m, in \u001b[0;36m_TFRunBuilder.get\u001b[1;34m(self, to_fetch)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     50\u001b[0m         logger\u001b[39m.\u001b[39mexception(\n\u001b[0;32m     51\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mError fetching: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, feed_dict=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     52\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict\n\u001b[0;32m     53\u001b[0m             )\n\u001b[0;32m     54\u001b[0m         )\n\u001b[1;32m---> 55\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(to_fetch, \u001b[39mint\u001b[39m):\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed[to_fetch]\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py:42\u001b[0m, in \u001b[0;36m_TFRunBuilder.get\u001b[1;34m(self, to_fetch)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39m=\u001b[39m _run_timeline(\n\u001b[0;32m     43\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession,\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetches,\n\u001b[0;32m     45\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug_name,\n\u001b[0;32m     46\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_dict,\n\u001b[0;32m     47\u001b[0m             os\u001b[39m.\u001b[39;49menviron\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mTF_TIMELINE_DIR\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     48\u001b[0m         )\n\u001b[0;32m     49\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     50\u001b[0m         logger\u001b[39m.\u001b[39mexception(\n\u001b[0;32m     51\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mError fetching: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, feed_dict=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     52\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict\n\u001b[0;32m     53\u001b[0m             )\n\u001b[0;32m     54\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\ray\\rllib\\utils\\tf_run_builder.py:112\u001b[0m, in \u001b[0;36m_run_timeline\u001b[1;34m(sess, ops, debug_name, feed_dict, timeline_dir)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39mtf_timeline\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    108\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m    109\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExecuting TF run without tracing. To dump TF timeline traces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mto disk, set the TF_TIMELINE_DIR environment variable.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         )\n\u001b[1;32m--> 112\u001b[0m     fetches \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun(ops, feed_dict\u001b[39m=\u001b[39;49mfeed_dict)\n\u001b[0;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m fetches\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\doore\\anaconda3\\envs\\rllib\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1165\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1161\u001b[0m   np_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(subfeed_val, dtype\u001b[39m=\u001b[39msubfeed_dtype)\n\u001b[0;32m   1163\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_tensor_handle_feed \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39mnot\u001b[39;00m subfeed_t\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mis_compatible_with(np_val\u001b[39m.\u001b[39mshape)):\n\u001b[1;32m-> 1165\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1166\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot feed value of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(np_val\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m for Tensor \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1167\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, which has shape \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1168\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(subfeed_t\u001b[39m.\u001b[39mget_shape())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mis_feedable(subfeed_t):\n\u001b[0;32m   1170\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensor \u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m may not be fed.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 32) for Tensor var_scope_1/state_in_0:0, which has shape (?, ?, 32)"
     ]
    }
   ],
   "source": [
    "_state = policy['default_policy'].get_initial_state()\n",
    "_state = np.tile(_state,(10,1))\n",
    "# _state = np.concatenate(_state)\n",
    "\n",
    "env.close()\n",
    "# _obs, _ = env.reset()\n",
    "\n",
    "_act, _state, _ = policy['default_policy'].compute_single_action(obs=_obs, state=_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
