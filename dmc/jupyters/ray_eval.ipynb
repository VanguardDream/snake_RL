{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bong\\anaconda3\\envs\\ray\\lib\\site-packages\\ray\\tune\\logger\\tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pathlib\n",
    "# # __location__ = os.getcwd()\n",
    "# # __location__ = pathlib.Path(__location__)\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\bong\\\\project\\\\snake_RL\\\\dmc\\\\domains')\n",
    "# import snake\n",
    "# import snake_v2\n",
    "# import snake_v3\n",
    "\n",
    "\n",
    "# from snake.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "# from snake_v2.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "from snake_v8.envs.SnakeEnv import SnakeEnv # for ray env register\n",
    "import gymnasium as gym\n",
    "\n",
    "# import rl 알고리즘\n",
    "from ray.rllib.algorithms.ppo import PPOConfig \n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Snake Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('snake/SnakeEnv-v8', render_mode=\"human\")\n",
    "\n",
    "register_env(\"snake-v8\", lambda config: SnakeEnv())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.rllib.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mannotations\u001b[39;00m \u001b[39mimport\u001b[39;00m override\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m try_import_tf, try_import_torch\n\u001b[1;32m---> 10\u001b[0m load_policy \u001b[39m=\u001b[39m Policy\u001b[39m.\u001b[39;49mfrom_checkpoint(\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mBong\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mray_results\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPPO_snake-v8_Base_925-1050\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mcheckpoint_001050\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Bong\\anaconda3\\envs\\ray\\lib\\site-packages\\ray\\rllib\\policy\\policy.py:256\u001b[0m, in \u001b[0;36mPolicy.from_checkpoint\u001b[1;34m(checkpoint, policy_ids)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[39massert\u001b[39;00m policy_checkpoint_info[\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPolicy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(policy_checkpoint_info[\u001b[39m\"\u001b[39m\u001b[39mstate_file\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 256\u001b[0m                 policy_state \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m    257\u001b[0m             policies[policy_id] \u001b[39m=\u001b[39m Policy\u001b[39m.\u001b[39mfrom_state(policy_state)\n\u001b[0;32m    258\u001b[0m \u001b[39mreturn\u001b[39;00m policies\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ray.rllib.core'"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.recurrent_net import RecurrentNetwork as TorchRNN\n",
    "from ray.rllib.models.tf.recurrent_net import RecurrentNetwork\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "load_policy = Policy.from_checkpoint('C:\\\\Users\\\\Bong\\\\ray_results\\\\PPO_snake-v8_Base_925-1050\\\\checkpoint_001050')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_policy = Policy.from_checkpoint('C:\\\\Users\\\\Bong\\\\ray_results\\\\PPO_snake-v8_Base_925-1050\\\\checkpoint_001050')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Recurrent Policy\n",
    "\n",
    "# lstm_dim = 64\n",
    "# num_lstm_cells = 2\n",
    "# initial_state = np.zeros((num_lstm_cells, lstm_dim))\n",
    "\n",
    "_state = load_policy['default_policy'].get_initial_state()\n",
    "_prev_action = np.zeros(14,)\n",
    "_reward = 0\n",
    "_obs, _ = env.reset()\n",
    "_accum_reward = 0\n",
    "\n",
    "for epi in range(5):\n",
    "    for i in range(1870):\n",
    "        _act, _state, _ = load_policy['default_policy'].compute_single_action(obs=_obs.copy(), state=_state.copy(), prev_action=_prev_action.copy(), prev_reward=_reward)\n",
    "        _prev_action = _act.copy()\n",
    "        # print(_act)\n",
    "        _obs, _reward, _done, _, _dict = env.step(_act)\n",
    "        _accum_reward = _accum_reward + _reward\n",
    "\n",
    "        if _done:\n",
    "            break\n",
    "\n",
    "        # print(str(_accum_reward)+\"  \\r\",end='')\n",
    "        # print(f\"{_dict['head_rotation']}\")\n",
    "        # print(_dict['forward_reward'])\n",
    "    _obs, _ = env.reset()\n",
    "    _accum_reward = 0\n",
    "    print(\"episode done...\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_policy['default_policy'].model.get_parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_obs, _ = env.reset()\n",
    "\n",
    "c_reward = 0\n",
    "for epi in range(10):\n",
    "    for i in range(1400):\n",
    "        _act = load_policy['default_policy'].compute_single_action(_obs)\n",
    "        _obs, _reward, _, _, _ = env.step(_act[0])\n",
    "        c_reward = c_reward + _reward\n",
    "    _obs, _ = env.reset()\n",
    "    print(c_reward)\n",
    "    c_reward = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_state = load_policy['default_policy'].get_initial_state()\n",
    "_state = np.tile(_state,(10,1))\n",
    "# _state = np.concatenate(_state)\n",
    "\n",
    "env.close()\n",
    "# _obs, _ = env.reset()\n",
    "\n",
    "_act, _state, _ = load_policy['default_policy'].compute_single_action(obs=_obs, state=_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d47bedaa1c91cdd818b14831e10e7cdc4f0653b726a0327bbea2da1503325d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
